{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c40ff1",
   "metadata": {},
   "source": [
    "TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87061189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All ML libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, \n",
    "    GradientBoostingRegressor,\n",
    "    VotingRegressor,\n",
    "    StackingRegressor\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "print(\"✅ All ML libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4362d62c",
   "metadata": {},
   "source": [
    "DEFINE PREPROCESSOR CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8da58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RealEstatePreprocessor class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class RealEstatePreprocessor:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for real estate data.\n",
    "    Prevents data leakage by fitting only on training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_folds=5, random_state=42):\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Store encoders and scalers (FITTED ON TRAIN ONLY!)\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.target_encodings = {}\n",
    "        self.fill_values = {}\n",
    "        self.feature_columns = None  # CRITICAL: Store feature order\n",
    "        self.scaled_columns = None   # CRITICAL: Store which columns were scaled\n",
    "        \n",
    "        # Features to drop (leakage)\n",
    "        self.leakage_cols = ['Price_per_m2', 'Price_category', 'Area_category', 'Ward']\n",
    "        \n",
    "        # Location scores\n",
    "        self.city_base_scores = {\n",
    "            'Hồ Chí Minh': 50,\n",
    "            'Hà Nội': 50,\n",
    "        }\n",
    "        \n",
    "        self.district_scores = {\n",
    "            'Hồ Chí Minh': {\n",
    "                'Quận 1': 10, 'Quận 3': 9, 'Bình Thạnh': 8, 'Phú Nhuận': 8,\n",
    "                'Quận 2': 7, 'Quận 7': 7, 'Quận 10': 7, 'Tân Bình': 7,\n",
    "                'Quận 5': 6, 'Quận 6': 6, 'Gò Vấp': 6, 'Quận 8': 5,\n",
    "                'Quận 9': 5, 'Thủ Đức': 5, 'Quận 12': 4, 'Tân Phú': 6,\n",
    "                'Bình Tân': 4, 'Bình Chánh': 4, 'Hóc Môn': 3,\n",
    "                'Củ Chi': 2, 'Nhà Bè': 3, 'Cần Giờ': 1\n",
    "            },\n",
    "            'Hà Nội': {\n",
    "                'Hoàn Kiếm': 10, 'Ba Đình': 9, 'Đống Đa': 8, 'Hai Bà Trưng': 8,\n",
    "                'Cầu Giấy': 7, 'Thanh Xuân': 7, 'Tây Hồ': 7,\n",
    "                'Long Biên': 6, 'Hoàng Mai': 6, 'Nam Từ Liêm': 6, 'Bắc Từ Liêm': 6,\n",
    "                'Hà Đông': 5, 'Đông Anh': 4, 'Gia Lâm': 4, 'Thanh Trì': 4,\n",
    "                'Sóc Sơn': 3, 'Ba Vì': 2, 'Mỹ Đức': 2, 'Chương Mỹ': 2,\n",
    "                'Thường Tín': 3, 'Mê Linh': 3, 'Hoài Đức': 3, 'Thạch Thất': 2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def clean_strings(self, df):\n",
    "        \"\"\"Step 1: Normalize string columns\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 1: STRING NORMALIZATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        string_cols = ['City', 'District']\n",
    "        for col in string_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].str.strip().str.title()\n",
    "                df[col] = df[col].str.replace(r'\\.$', '', regex=True)\n",
    "                print(f\"{col} unique values: {df[col].nunique()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_leakage(self, df):\n",
    "        \"\"\"Step 2: Remove leakage columns\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 2: REMOVE LEAKAGE FEATURES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        cols_to_drop = [col for col in self.leakage_cols if col in df.columns]\n",
    "        if cols_to_drop:\n",
    "            print(f\"Dropping: {cols_to_drop}\")\n",
    "            df = df.drop(columns=cols_to_drop)\n",
    "        return df\n",
    "    \n",
    "    def handle_missing(self, df, is_train=True):\n",
    "        \"\"\"Step 3: Handle missing values\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 3: HANDLE MISSING VALUES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Numeric: fill with median\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if df[col].isna().sum() > 0:\n",
    "                if is_train:\n",
    "                    self.fill_values[col] = df[col].median()\n",
    "                fill_val = self.fill_values.get(col, df[col].median())\n",
    "                df[col].fillna(fill_val, inplace=True)\n",
    "                if is_train:\n",
    "                    print(f\"✓ {col}: filled with TRAIN median {fill_val:.2f}\")\n",
    "                else:\n",
    "                    print(f\"✓ {col}: filled with STORED median {fill_val:.2f}\")\n",
    "        \n",
    "        # Categorical: fill with 'Unknown'\n",
    "        cat_cols = df.select_dtypes(include=['object']).columns\n",
    "        for col in cat_cols:\n",
    "            if df[col].isna().sum() > 0:\n",
    "                df[col].fillna('Unknown', inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_bad_records(self, df):\n",
    "        \"\"\"Step 4: Remove invalid records\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 4: REMOVE BAD RECORDS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        initial_shape = df.shape[0]\n",
    "        df = df[(df['Price'] > 0) & (df['Area'] > 0)]\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        print(f\"Removed {initial_shape - df.shape[0]} bad records\")\n",
    "        return df\n",
    "    \n",
    "    def handle_outliers(self, df, method='percentile'):\n",
    "        \"\"\"Step 5: Handle outliers\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 5: HANDLE OUTLIERS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        cols = ['Price', 'Area', 'Frontage', 'Access Road']\n",
    "        \n",
    "        for col in cols:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            lower = df[col].quantile(0.01)\n",
    "            upper = df[col].quantile(0.99)\n",
    "            df[col] = df[col].clip(lower, upper)\n",
    "            print(f\"{col}: capped to [{lower:.2f}, {upper:.2f}]\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"Step 6: Feature engineering\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 6: FEATURE ENGINEERING\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Basic features\n",
    "        if 'Total_rooms' not in df.columns:\n",
    "            df['Total_rooms'] = df['Bedrooms'] + df['Bathrooms']\n",
    "        \n",
    "        df['Bedroom_Bathroom_ratio'] = df['Bedrooms'] / df['Bathrooms'].replace(0, 1)\n",
    "        df['Area_per_floor'] = df['Area'] / df['Floors'].replace(0, 1)\n",
    "        df['Room_density'] = df['Total_rooms'] / df['Area']\n",
    "        \n",
    "        # Luxury score\n",
    "        binary_features = ['Has_Frontage', 'Has_Access_Road', \n",
    "                          'Has_House_Direction', 'Has_Balcony_Direction']\n",
    "        df['Luxury_score'] = sum(df[col] for col in binary_features if col in df.columns)\n",
    "        \n",
    "        # Location features\n",
    "        df['City_Base_Score'] = df['City'].map(self.city_base_scores).fillna(40)\n",
    "        \n",
    "        # District score (no lambda - pickle-safe)\n",
    "        district_scores = []\n",
    "        for idx, row in df.iterrows():\n",
    "            city = row['City']\n",
    "            district = row['District']\n",
    "            score = self.district_scores.get(city, {}).get(district, 3)\n",
    "            district_scores.append(score)\n",
    "        df['District_Score'] = district_scores\n",
    "        \n",
    "        df['Location_Score'] = df['City_Base_Score'] + df['District_Score']\n",
    "        df['Location_Tier'] = pd.cut(\n",
    "            df['Location_Score'],\n",
    "            bins=[0, 50, 55, 60, 100],\n",
    "            labels=['Suburban', 'Urban', 'Premium', 'Elite']\n",
    "        )\n",
    "        \n",
    "        # Other features\n",
    "        df['Is_Apartment'] = df['Address'].str.contains('Dự án|Project', case=False, na=False).astype(int)\n",
    "        df['Full_Legal'] = (df['Legal status'] == 'Have Certificate').astype(int)\n",
    "        df['Full_Furniture'] = (df['Furniture state'] == 'Full').astype(int)\n",
    "        \n",
    "        print(f\"✓ Created 13 new features\")\n",
    "        return df\n",
    "    \n",
    "    def target_encode_kfold(self, df, col, target='Price', is_train=True):\n",
    "        \"\"\"Target encoding with K-Fold (prevents leakage)\"\"\"\n",
    "        if is_train:\n",
    "            # Initialize column\n",
    "            df[f'{col}_target_enc'] = 0.0\n",
    "            \n",
    "            kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "            \n",
    "            # Use iloc instead of loc to avoid index issues\n",
    "            for train_idx, val_idx in kf.split(df):\n",
    "                # Calculate mean target per category on train fold\n",
    "                train_data = df.iloc[train_idx]\n",
    "                target_means = train_data.groupby(col)[target].mean()\n",
    "                global_mean = train_data[target].mean()\n",
    "                \n",
    "                # Apply to validation fold using iloc\n",
    "                val_data = df.iloc[val_idx]\n",
    "                encoded_values = val_data[col].map(target_means).fillna(global_mean)\n",
    "                df.iloc[val_idx, df.columns.get_loc(f'{col}_target_enc')] = encoded_values.values\n",
    "            \n",
    "            # Store global encoding for test data\n",
    "            self.target_encodings[col] = df.groupby(col)[target].mean().to_dict()\n",
    "            self.target_encodings[f'{col}_global_mean'] = df[target].mean()\n",
    "            print(f\"  ✓ {col}: K-Fold encoded (train)\")\n",
    "        else:\n",
    "            global_mean = self.target_encodings.get(f'{col}_global_mean', 0)\n",
    "            df[f'{col}_target_enc'] = df[col].map(self.target_encodings[col]).fillna(global_mean)\n",
    "            print(f\"  ✓ {col}: Applied stored encoding (test)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def encode_categoricals(self, df, is_train=True):\n",
    "        \"\"\"Step 7: Encode categorical variables\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 7: ENCODE CATEGORICAL FEATURES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Target encoding\n",
    "        df = self.target_encode_kfold(df, 'District', is_train=is_train)\n",
    "        df = self.target_encode_kfold(df, 'City', is_train=is_train)\n",
    "        \n",
    "        # Label encoding\n",
    "        label_cols = ['Legal status', 'Furniture state', 'Location_Tier']\n",
    "        for col in label_cols:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            if is_train:\n",
    "                self.label_encoders[col] = LabelEncoder()\n",
    "                df[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df[col].astype(str))\n",
    "                print(f\"  ✓ {col}: Label encoded (train)\")\n",
    "            else:\n",
    "                # Encode using stored encoder (no lambda - pickle-safe)\n",
    "                encoded_values = []\n",
    "                for value in df[col].astype(str):\n",
    "                    if value in self.label_encoders[col].classes_:\n",
    "                        encoded_values.append(self.label_encoders[col].transform([value])[0])\n",
    "                    else:\n",
    "                        encoded_values.append(-1)\n",
    "                df[f'{col}_encoded'] = encoded_values\n",
    "                print(f\"  ✓ {col}: Applied stored encoding (test)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scale_features(self, df, is_train=True):\n",
    "        \"\"\"Step 8: Scale numeric features\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 8: SCALE NUMERIC FEATURES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        exclude = ['Price', 'Address']\n",
    "        \n",
    "        if is_train:\n",
    "            numeric_cols = [col for col in df.select_dtypes(include=[np.number]).columns \n",
    "                           if col not in exclude]\n",
    "            \n",
    "            df[numeric_cols] = self.scaler.fit_transform(df[numeric_cols])\n",
    "            \n",
    "            # Store which columns were scaled\n",
    "            self.scaled_columns = numeric_cols\n",
    "            \n",
    "            print(f\"✓ Fitted scaler on {len(numeric_cols)} features (TRAIN)\")\n",
    "            print(f\"✓ Stored scaled columns: {numeric_cols[:5]}...\")\n",
    "        else:\n",
    "            # Use stored columns from training\n",
    "            if self.scaled_columns is None:\n",
    "                raise ValueError(\"Scaler not fitted yet. Call fit_transform first.\")\n",
    "            \n",
    "            # Only scale columns that exist and were in training\n",
    "            cols_to_scale = [col for col in self.scaled_columns if col in df.columns]\n",
    "            \n",
    "            df[cols_to_scale] = self.scaler.transform(df[cols_to_scale])\n",
    "            print(f\"✓ Applied scaler to {len(cols_to_scale)} features (TEST)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Complete preprocessing for TRAINING data\"\"\"\n",
    "        print(\"\\n\" + \"🚀 \" * 20)\n",
    "        print(\"PREPROCESSING PIPELINE - TRAINING DATA\")\n",
    "        print(\"🚀 \" * 20)\n",
    "        \n",
    "        # CRITICAL: Reset index to avoid KeyError in K-Fold\n",
    "        df = df.reset_index(drop=True)\n",
    "        print(\"✓ Index reset for safe processing\")\n",
    "        \n",
    "        df = self.clean_strings(df)\n",
    "        df = self.remove_leakage(df)\n",
    "        df = self.handle_missing(df, is_train=True)\n",
    "        df = self.remove_bad_records(df)\n",
    "        df = self.handle_outliers(df)\n",
    "        df = self.engineer_features(df)\n",
    "        df = self.encode_categoricals(df, is_train=True)\n",
    "        \n",
    "        # Separate target\n",
    "        y = df['Price'].copy()\n",
    "        X = df.drop(columns=['Price', 'Address', 'City', 'District', \n",
    "                             'Legal status', 'Furniture state', 'Location_Tier','House direction', 'Balcony direction'], \n",
    "                    errors='ignore')\n",
    "        \n",
    "        X = self.scale_features(X, is_train=True)\n",
    "        \n",
    "        # CRITICAL: Store feature columns in order\n",
    "        self.feature_columns = list(X.columns)\n",
    "        \n",
    "        print(\"\\n\" + \"✅ \" * 20)\n",
    "        print(f\"FINAL SHAPE: X={X.shape}, y={y.shape}\")\n",
    "        print(f\"✅ Feature columns stored: {len(self.feature_columns)}\")\n",
    "        print(\"✅ \" * 20)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Apply preprocessing to TEST/NEW data.\n",
    "        \n",
    "        IMPORTANT: This method ALWAYS returns only X (features).\n",
    "        If you need y (target), extract it from the original df before calling this method.\n",
    "        \n",
    "        Returns:\n",
    "            X (pd.DataFrame): Preprocessed features\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"🔧 \" * 20)\n",
    "        print(\"PREPROCESSING PIPELINE - TEST/NEW DATA\")\n",
    "        print(\"🔧 \" * 20)\n",
    "        \n",
    "        # CRITICAL: Reset index to avoid KeyError\n",
    "        df = df.reset_index(drop=True)\n",
    "        print(\"✓ Index reset for safe processing\")\n",
    "        \n",
    "        df = self.clean_strings(df)\n",
    "        df = self.remove_leakage(df)\n",
    "        df = self.handle_missing(df, is_train=False)\n",
    "        df = self.engineer_features(df)\n",
    "        df = self.encode_categoricals(df, is_train=False)\n",
    "        \n",
    "        # Drop all non-feature columns\n",
    "        X = df.drop(columns=['Price', 'Address', 'City', 'District', \n",
    "                             'Legal status', 'Furniture state', 'Location_Tier',\n",
    "                             'House direction', 'Balcony direction'], \n",
    "                    errors='ignore')\n",
    "        X = self.scale_features(X, is_train=False)\n",
    "        \n",
    "        # CRITICAL: Ensure features match training (same columns, same order)\n",
    "        if self.feature_columns is not None:\n",
    "            missing_cols = set(self.feature_columns) - set(X.columns)\n",
    "            extra_cols = set(X.columns) - set(self.feature_columns)\n",
    "            \n",
    "            if missing_cols:\n",
    "                print(f\"⚠️  Adding missing features: {missing_cols}\")\n",
    "                for col in missing_cols:\n",
    "                    X[col] = 0\n",
    "            \n",
    "            if extra_cols:\n",
    "                print(f\"⚠️  Dropping extra features: {extra_cols}\")\n",
    "                X = X.drop(columns=list(extra_cols))\n",
    "            \n",
    "            # Reorder columns to match training exactly\n",
    "            X = X[self.feature_columns]\n",
    "            print(f\"✓ Features reordered to match training\")\n",
    "        \n",
    "        print(\"\\n\" + \"✅ \" * 20)\n",
    "        print(f\"FINAL SHAPE: X={X.shape}\")\n",
    "        print(f\"✓ Feature order verified: {list(X.columns)[:5]}...\")\n",
    "        print(\"✅ \" * 20)\n",
    "        \n",
    "        # ALWAYS return only X (no tuple, no conditional return)\n",
    "        return X\n",
    "\n",
    "print(\"✅ RealEstatePreprocessor class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06852a4d",
   "metadata": {},
   "source": [
    "DATA CLEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a81264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADED\n",
      "================================================================================\n",
      "Total records: 22,245\n",
      "Total features: 23\n",
      "\n",
      "First few rows:\n",
      "                                             Address  Area  Frontage  \\\n",
      "0  Đường Nguyễn Văn Khối, Phường 11, Gò Vấp, Hồ C...  54.0       4.0   \n",
      "1   Đường Quang Trung, Phường 8, Gò Vấp, Hồ Chí Minh  92.0       4.0   \n",
      "2  Dự án Him Lam Thường Tín, Huyện Thường Tín, Hà...  74.0       5.0   \n",
      "\n",
      "   Access Road  Floors  Bedrooms  Bathrooms      Legal status Furniture state  \\\n",
      "0          3.5     2.0       2.0        3.0  Have certificate            Full   \n",
      "1          5.0     2.0       4.0        4.0  Have certificate            Full   \n",
      "2         18.0     5.0       4.0        5.0  Have certificate            Full   \n",
      "\n",
      "   Price  ... Price_per_m2 Total_rooms Bedroom_Bathroom_ratio  Area_per_floor  \\\n",
      "0   5.35  ...     0.099074         5.0               0.666667            27.0   \n",
      "1   6.90  ...     0.075000         8.0               1.000000            46.0   \n",
      "2   9.90  ...     0.133784         9.0               0.800000            14.8   \n",
      "\n",
      "   Area_category  Price_category  Has_Frontage Has_Access_Road  \\\n",
      "0          Small           Cheap             0               1   \n",
      "1         Medium           Cheap             0               0   \n",
      "2          Small           Cheap             1               1   \n",
      "\n",
      "  Has_House_Direction  Has_Balcony_Direction  \n",
      "0                   1                      1  \n",
      "1                   1                      1  \n",
      "2                   1                      1  \n",
      "\n",
      "[3 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/housing_data_processed.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total records: {df.shape[0]:,}\")\n",
    "print(f\"Total features: {df.shape[1]}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f4dc2a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06fd5b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAIN-TEST SPLIT\n",
      "================================================================================\n",
      "📊 Training set: 17,796 records (80.0%)\n",
      "📊 Test set:     4,449 records (20.0%)\n",
      "\n",
      "Price distribution:\n",
      "  Train - Mean: 6.18, Median: 6.20\n",
      "  Test  - Mean: 6.22, Median: 6.20\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"📊 Training set: {train_df.shape[0]:,} records ({train_df.shape[0]/df.shape[0]*100:.1f}%)\")\n",
    "print(f\"📊 Test set:     {test_df.shape[0]:,} records ({test_df.shape[0]/df.shape[0]*100:.1f}%)\")\n",
    "print(f\"\\nPrice distribution:\")\n",
    "print(f\"  Train - Mean: {train_df['Price'].mean():.2f}, Median: {train_df['Price'].median():.2f}\")\n",
    "print(f\"  Test  - Mean: {test_df['Price'].mean():.2f}, Median: {test_df['Price'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb2392",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "149c3b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPROCESSOR INITIALIZED\n",
      "================================================================================\n",
      "✓ K-Fold splits: 5\n",
      "✓ Random state: 42\n",
      "✓ Leakage columns to remove: ['Price_per_m2', 'Price_category', 'Area_category', 'Ward']\n",
      "✓ Cities covered: ['Hồ Chí Minh', 'Hà Nội']\n"
     ]
    }
   ],
   "source": [
    "preprocessor = RealEstatePreprocessor(\n",
    "    n_folds=5,        # Number of folds for target encoding\n",
    "    random_state=42   # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PREPROCESSOR INITIALIZED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ K-Fold splits: {preprocessor.n_folds}\")\n",
    "print(f\"✓ Random state: {preprocessor.random_state}\")\n",
    "print(f\"✓ Leakage columns to remove: {preprocessor.leakage_cols}\")\n",
    "print(f\"✓ Cities covered: {list(preprocessor.city_base_scores.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6196b0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c378260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 \n",
      "PREPROCESSING PIPELINE - TRAINING DATA\n",
      "🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 \n",
      "✓ Index reset for safe processing\n",
      "\n",
      "============================================================\n",
      "STEP 1: STRING NORMALIZATION\n",
      "============================================================\n",
      "City unique values: 4\n",
      "District unique values: 73\n",
      "\n",
      "============================================================\n",
      "STEP 2: REMOVE LEAKAGE FEATURES\n",
      "============================================================\n",
      "Dropping: ['Price_per_m2', 'Price_category', 'Area_category', 'Ward']\n",
      "\n",
      "============================================================\n",
      "STEP 3: HANDLE MISSING VALUES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STEP 4: REMOVE BAD RECORDS\n",
      "============================================================\n",
      "Removed 24 bad records\n",
      "\n",
      "============================================================\n",
      "STEP 5: HANDLE OUTLIERS\n",
      "============================================================\n",
      "Price: capped to [1.68, 10.00]\n",
      "Area: capped to [20.00, 160.00]\n",
      "Frontage: capped to [3.00, 12.00]\n",
      "Access Road: capped to [2.00, 21.00]\n",
      "\n",
      "============================================================\n",
      "STEP 6: FEATURE ENGINEERING\n",
      "============================================================\n",
      "✓ Created 13 new features\n",
      "\n",
      "============================================================\n",
      "STEP 7: ENCODE CATEGORICAL FEATURES\n",
      "============================================================\n",
      "  ✓ District: K-Fold encoded (train)\n",
      "  ✓ City: K-Fold encoded (train)\n",
      "  ✓ Legal status: Label encoded (train)\n",
      "  ✓ Furniture state: Label encoded (train)\n",
      "  ✓ Location_Tier: Label encoded (train)\n",
      "\n",
      "============================================================\n",
      "STEP 8: SCALE NUMERIC FEATURES\n",
      "============================================================\n",
      "✓ Fitted scaler on 26 features (TRAIN)\n",
      "\n",
      "✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ \n",
      "FINAL SHAPE: X=(17772, 26), y=(17772,)\n",
      "✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ \n",
      "\n",
      "================================================================================\n",
      "TRAINING DATA PREPROCESSING COMPLETE\n",
      "================================================================================\n",
      "✅ X_train shape: (17772, 26)\n",
      "✅ y_train shape: (17772,)\n",
      "\n",
      "Feature names (26):\n",
      "   1. Area\n",
      "   2. Frontage\n",
      "   3. Access Road\n",
      "   4. Floors\n",
      "   5. Bedrooms\n",
      "   6. Bathrooms\n",
      "   7. Total_rooms\n",
      "   8. Bedroom_Bathroom_ratio\n",
      "   9. Area_per_floor\n",
      "  10. Has_Frontage\n",
      "  11. Has_Access_Road\n",
      "  12. Has_House_Direction\n",
      "  13. Has_Balcony_Direction\n",
      "  14. Room_density\n",
      "  15. Luxury_score\n",
      "  16. City_Base_Score\n",
      "  17. District_Score\n",
      "  18. Location_Score\n",
      "  19. Is_Apartment\n",
      "  20. Full_Legal\n",
      "  21. Full_Furniture\n",
      "  22. District_target_enc\n",
      "  23. City_target_enc\n",
      "  24. Legal status_encoded\n",
      "  25. Furniture state_encoded\n",
      "  26. Location_Tier_encoded\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = preprocessor.fit_transform(train_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING DATA PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✅ X_train shape: {X_train.shape}\")\n",
    "print(f\"✅ y_train shape: {y_train.shape}\")\n",
    "print(f\"\\nFeature names ({len(X_train.columns)}):\")\n",
    "for i, col in enumerate(X_train.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e668dd3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64a0e9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 \n",
      "PREPROCESSING PIPELINE - TEST/NEW DATA\n",
      "🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 \n",
      "✓ Index reset for safe processing\n",
      "\n",
      "============================================================\n",
      "STEP 1: STRING NORMALIZATION\n",
      "============================================================\n",
      "City unique values: 5\n",
      "District unique values: 63\n",
      "\n",
      "============================================================\n",
      "STEP 2: REMOVE LEAKAGE FEATURES\n",
      "============================================================\n",
      "Dropping: ['Price_per_m2', 'Price_category', 'Area_category', 'Ward']\n",
      "\n",
      "============================================================\n",
      "STEP 3: HANDLE MISSING VALUES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STEP 6: FEATURE ENGINEERING\n",
      "============================================================\n",
      "✓ Created 13 new features\n",
      "\n",
      "============================================================\n",
      "STEP 7: ENCODE CATEGORICAL FEATURES\n",
      "============================================================\n",
      "  ✓ District: Applied stored encoding (test)\n",
      "  ✓ City: Applied stored encoding (test)\n",
      "  ✓ Legal status: Applied stored encoding (test)\n",
      "  ✓ Furniture state: Applied stored encoding (test)\n",
      "  ✓ Location_Tier: Applied stored encoding (test)\n",
      "\n",
      "============================================================\n",
      "STEP 8: SCALE NUMERIC FEATURES\n",
      "============================================================\n",
      "✓ Applied scaler to 26 features (TEST)\n",
      "\n",
      "✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ \n",
      "FINAL SHAPE: X=(4449, 26)\n",
      "✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ \n",
      "\n",
      "================================================================================\n",
      "TEST DATA PREPROCESSING COMPLETE\n",
      "================================================================================\n",
      "✅ X_test shape: (4449, 26)\n",
      "✅ y_test shape: (4449,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_test = test_df['Price'].copy()\n",
    "\n",
    "# Transform chỉ trả về X\n",
    "X_test = preprocessor.transform(test_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST DATA PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✅ X_test shape: {X_test.shape}\")\n",
    "print(f\"✅ y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0871140",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2f39ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 \n",
      "DATA LEAKAGE VERIFICATION (CORRECTED)\n",
      "🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 \n",
      "\n",
      "1. Feature consistency:\n",
      "   ✅ Train and test have SAME features\n",
      "   Total features: 26\n",
      "\n",
      "2. Target variable:\n",
      "   ✅ Price NOT in features (correct!)\n",
      "\n",
      "3. Stored preprocessing values:\n",
      "   ✅ Fill values stored: 0 numeric columns\n",
      "   ✅ Label encoders stored: 3 categorical columns\n",
      "      Columns: ['Legal status', 'Furniture state', 'Location_Tier']\n",
      "   ✅ Target encodings stored: 2 columns\n",
      "      Columns: ['District', 'City']\n",
      "   ✅ Scaler fitted: 26 features\n",
      "\n",
      "4. Target encoding verification:\n",
      "   Checking that test uses stored train encodings...\n",
      "\n",
      "   Stored District encodings (from train):\n",
      "      Ba Vì: 8.43\n",
      "      Ba Đình: 7.29\n",
      "      Bình Chánh: 5.28\n",
      "\n",
      "   Verification for District='Ba Đình':\n",
      "      Stored (from train): 7.294094\n",
      "      Applied (in test):   1.397555\n",
      "      ❌ MISMATCH! Possible leakage!\n",
      "\n",
      "5. Scaler verification:\n",
      "   Train data range (sample feature):\n",
      "      Area: [-1.3661, 4.0915]\n",
      "   Test data range (same feature):\n",
      "      Area: [-2.0248, 17.3454]\n",
      "   ℹ️  Test range can exceed train range (this is normal)\n",
      "\n",
      "6. Categorical columns removed:\n",
      "   ✅ Original categorical columns removed\n",
      "      Removed: ['City', 'District', 'Legal status', 'Furniture state', 'Location_Tier']\n",
      "\n",
      "7. Leakage columns removed:\n",
      "   ✅ All leakage columns removed\n",
      "      Target leakage columns: ['Price_per_m2', 'Price_category', 'Area_category', 'Ward']\n",
      "\n",
      "============================================================\n",
      "VERIFICATION SUMMARY\n",
      "============================================================\n",
      "⚠️  Some checks failed - review warnings above\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"🔍 \" * 20)\n",
    "print(\"DATA LEAKAGE VERIFICATION (CORRECTED)\")\n",
    "print(\"🔍 \" * 20)\n",
    "\n",
    "# Check 1: Feature names match\n",
    "print(\"\\n1. Feature consistency:\")\n",
    "if list(X_train.columns) == list(X_test.columns):\n",
    "    print(\"   ✅ Train and test have SAME features\")\n",
    "    print(f\"   Total features: {len(X_train.columns)}\")\n",
    "else:\n",
    "    print(\"   ❌ WARNING: Feature mismatch!\")\n",
    "    missing_in_test = set(X_train.columns) - set(X_test.columns)\n",
    "    missing_in_train = set(X_test.columns) - set(X_train.columns)\n",
    "    if missing_in_test:\n",
    "        print(f\"   Missing in test: {missing_in_test}\")\n",
    "    if missing_in_train:\n",
    "        print(f\"   Missing in train: {missing_in_train}\")\n",
    "\n",
    "# Check 2: No Price in features\n",
    "print(\"\\n2. Target variable:\")\n",
    "if 'Price' not in X_train.columns and 'Price' not in X_test.columns:\n",
    "    print(\"   ✅ Price NOT in features (correct!)\")\n",
    "else:\n",
    "    print(\"   ❌ WARNING: Price found in features!\")\n",
    "\n",
    "# Check 3: Verify stored values\n",
    "print(\"\\n3. Stored preprocessing values:\")\n",
    "print(f\"   ✅ Fill values stored: {len(preprocessor.fill_values)} numeric columns\")\n",
    "if len(preprocessor.fill_values) > 0:\n",
    "    print(f\"      Example: {list(preprocessor.fill_values.items())[:2]}\")\n",
    "\n",
    "print(f\"   ✅ Label encoders stored: {len(preprocessor.label_encoders)} categorical columns\")\n",
    "if len(preprocessor.label_encoders) > 0:\n",
    "    print(f\"      Columns: {list(preprocessor.label_encoders.keys())}\")\n",
    "\n",
    "target_enc_cols = [k for k in preprocessor.target_encodings.keys() if not k.endswith('_global_mean')]\n",
    "print(f\"   ✅ Target encodings stored: {len(target_enc_cols)} columns\")\n",
    "if len(target_enc_cols) > 0:\n",
    "    print(f\"      Columns: {target_enc_cols}\")\n",
    "\n",
    "print(f\"   ✅ Scaler fitted: {preprocessor.scaler.n_features_in_} features\")\n",
    "\n",
    "# Check 4: CORRECTED - Compare stored encoding values directly\n",
    "print(\"\\n4. Target encoding verification:\")\n",
    "print(\"   Checking that test uses stored train encodings...\")\n",
    "\n",
    "if 'District' in preprocessor.target_encodings:\n",
    "    district_encodings = preprocessor.target_encodings['District']\n",
    "    \n",
    "    # Show sample of stored encodings\n",
    "    sample_districts = list(district_encodings.items())[:3]\n",
    "    print(f\"\\n   Stored District encodings (from train):\")\n",
    "    for district, encoding in sample_districts:\n",
    "        print(f\"      {district}: {encoding:.2f}\")\n",
    "    \n",
    "    # Verify test data uses these exact values\n",
    "    if 'District_target_enc' in X_test.columns:\n",
    "        # Get a sample district from test_df that exists in training\n",
    "        common_districts = set(test_df['District']) & set(district_encodings.keys())\n",
    "        if common_districts:\n",
    "            sample_district = list(common_districts)[0]\n",
    "            stored_encoding = district_encodings[sample_district]\n",
    "            \n",
    "            # Find this encoding in X_test\n",
    "            test_indices = test_df.index[test_df['District'] == sample_district].tolist()\n",
    "            if test_indices:\n",
    "                # Get the first matching index in test_df\n",
    "                original_idx = test_indices[0]\n",
    "                # Find corresponding position in X_test (which was reset to 0-based index)\n",
    "                position_in_test = test_df.index.get_loc(original_idx)\n",
    "                actual_test_encoding = X_test.iloc[position_in_test]['District_target_enc']\n",
    "                \n",
    "                print(f\"\\n   Verification for District='{sample_district}':\")\n",
    "                print(f\"      Stored (from train): {stored_encoding:.6f}\")\n",
    "                print(f\"      Applied (in test):   {actual_test_encoding:.6f}\")\n",
    "                \n",
    "                if abs(stored_encoding - actual_test_encoding) < 1e-6:\n",
    "                    print(f\"      ✅ MATCH! Test uses train encoding\")\n",
    "                else:\n",
    "                    print(f\"      ❌ MISMATCH! Possible leakage!\")\n",
    "\n",
    "# Check 5: Scaler verification\n",
    "print(\"\\n5. Scaler verification:\")\n",
    "print(f\"   Train data range (sample feature):\")\n",
    "sample_feature = X_train.columns[0]\n",
    "print(f\"      {sample_feature}: [{X_train[sample_feature].min():.4f}, {X_train[sample_feature].max():.4f}]\")\n",
    "print(f\"   Test data range (same feature):\")\n",
    "print(f\"      {sample_feature}: [{X_test[sample_feature].min():.4f}, {X_test[sample_feature].max():.4f}]\")\n",
    "print(f\"   ℹ️  Test range can exceed train range (this is normal)\")\n",
    "\n",
    "# Check 6: No raw categorical columns in features\n",
    "print(\"\\n6. Categorical columns removed:\")\n",
    "categorical_originals = ['City', 'District', 'Legal status', 'Furniture state', 'Location_Tier']\n",
    "found_in_train = [col for col in categorical_originals if col in X_train.columns]\n",
    "found_in_test = [col for col in categorical_originals if col in X_test.columns]\n",
    "\n",
    "if not found_in_train and not found_in_test:\n",
    "    print(f\"   ✅ Original categorical columns removed\")\n",
    "    print(f\"      Removed: {categorical_originals}\")\n",
    "else:\n",
    "    print(f\"   ❌ WARNING: Original categorical columns still present!\")\n",
    "    if found_in_train:\n",
    "        print(f\"      In train: {found_in_train}\")\n",
    "    if found_in_test:\n",
    "        print(f\"      In test: {found_in_test}\")\n",
    "\n",
    "# Check 7: Leakage columns removed\n",
    "print(\"\\n7. Leakage columns removed:\")\n",
    "leakage_in_train = [col for col in preprocessor.leakage_cols if col in X_train.columns]\n",
    "leakage_in_test = [col for col in preprocessor.leakage_cols if col in X_test.columns]\n",
    "\n",
    "if not leakage_in_train and not leakage_in_test:\n",
    "    print(f\"   ✅ All leakage columns removed\")\n",
    "    print(f\"      Target leakage columns: {preprocessor.leakage_cols}\")\n",
    "else:\n",
    "    print(f\"   ❌ WARNING: Leakage columns still present!\")\n",
    "    if leakage_in_train:\n",
    "        print(f\"      In train: {leakage_in_train}\")\n",
    "    if leakage_in_test:\n",
    "        print(f\"      In test: {leakage_in_test}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_checks_passed = (\n",
    "    list(X_train.columns) == list(X_test.columns) and\n",
    "    'Price' not in X_train.columns and\n",
    "    'Price' not in X_test.columns and\n",
    "    len(preprocessor.fill_values) > 0 and\n",
    "    len(preprocessor.label_encoders) > 0 and\n",
    "    not found_in_train and\n",
    "    not found_in_test and\n",
    "    not leakage_in_train and\n",
    "    not leakage_in_test\n",
    ")\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(\"✅ ALL CHECKS PASSED - No data leakage detected!\")\n",
    "    print(\"✅ Ready for model training\")\n",
    "else:\n",
    "    print(\"⚠️  Some checks failed - review warnings above\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81424200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessor updated with feature_columns from metadata!\n"
     ]
    }
   ],
   "source": [
    "# Thêm feature_columns vào preprocessor hiện tại\n",
    "with open('../models/best_model_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "preprocessor.feature_columns = metadata['feature_names']\n",
    "\n",
    "# Lưu lại\n",
    "joblib.dump(preprocessor, '../models/preprocessor.joblib', compress=3)\n",
    "print(\"✅ Preprocessor updated with feature_columns from metadata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62ec76",
   "metadata": {},
   "source": [
    "DEFINE EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caf92ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive regression metrics\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    \n",
    "    # Adjusted R²\n",
    "    n = len(y_true)\n",
    "    p = X_train.shape[1]  # number of features\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2,\n",
    "        'Adj_R²': adj_r2,\n",
    "        'MAPE (%)': mape\n",
    "    }\n",
    "\n",
    "\n",
    "def print_metrics(metrics_dict):\n",
    "    \"\"\"Pretty print metrics\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"📊 {metrics_dict['Model']} Performance\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  MAE (Mean Absolute Error):        {metrics_dict['MAE']:>15,.2f}\")\n",
    "    print(f\"  RMSE (Root Mean Squared Error):   {metrics_dict['RMSE']:>15,.2f}\")\n",
    "    print(f\"  R² Score:                         {metrics_dict['R²']:>15.4f}\")\n",
    "    print(f\"  Adjusted R²:                      {metrics_dict['Adj_R²']:>15.4f}\")\n",
    "    print(f\"  MAPE (Mean Abs Percentage Error): {metrics_dict['MAPE (%)']:>15.2f}%\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde8233",
   "metadata": {},
   "source": [
    "HYPERPARAMETER TUNING WITH OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43f0951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 \n",
      "HYPERPARAMETER TUNING WITH OPTUNA\n",
      "🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"🔬 \" * 20)\n",
    "print(\"HYPERPARAMETER TUNING WITH OPTUNA\")\n",
    "print(\"🔬 \" * 20)\n",
    "\n",
    "def tune_with_optuna(model_name, model_class, n_trials=50):\n",
    "    \"\"\"\n",
    "    Tune hyperparameters using Optuna\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Tuning {model_name} with Optuna\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Trials: {n_trials}\")\n",
    "    print(f\"  Metric: MAE (lower is better)\")\n",
    "    print(f\"  Sampler: TPE (Tree-structured Parzen Estimator)\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        if model_name == 'Ridge_Regression':\n",
    "            params = {\n",
    "                'alpha': trial.suggest_float('alpha', 0.01, 10.0, log=True),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            model = Ridge(**params)\n",
    "            \n",
    "        elif model_name == 'Lasso_Regression':\n",
    "            params = {\n",
    "                'alpha': trial.suggest_float('alpha', 0.01, 10.0, log=True),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            model = Lasso(**params)\n",
    "            \n",
    "        elif model_name == 'Random_Forest':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "                'max_depth': trial.suggest_int('max_depth', 10, 30),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = RandomForestRegressor(**params)\n",
    "            \n",
    "        elif model_name == 'Gradient_Boosting':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            model = GradientBoostingRegressor(**params)\n",
    "            \n",
    "        elif model_name == 'XGBoost':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 1.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 1.0),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = XGBRegressor(**params)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        return mae\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=TPESampler(seed=42)\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Optimize with progress bar suppression\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "    tuning_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"  ✅ Tuning completed in {tuning_time:.2f}s\")\n",
    "    print(f\"  🎯 Best MAE: {study.best_value:,.2f}\")\n",
    "    print(f\"  🎯 Best Trial: #{study.best_trial.number + 1}\")\n",
    "    print(f\"  📋 Best Parameters:\")\n",
    "    for param, value in study.best_params.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"      {param}: {value:.6f}\")\n",
    "        else:\n",
    "            print(f\"      {param}: {value}\")\n",
    "    \n",
    "    # Create model with best parameters\n",
    "    if model_name == 'Ridge_Regression':\n",
    "        best_model = Ridge(**study.best_params, random_state=42)\n",
    "    elif model_name == 'Lasso_Regression':\n",
    "        best_model = Lasso(**study.best_params, random_state=42)\n",
    "    elif model_name == 'Random_Forest':\n",
    "        best_model = RandomForestRegressor(**study.best_params, random_state=42, n_jobs=-1)\n",
    "    elif model_name == 'Gradient_Boosting':\n",
    "        best_model = GradientBoostingRegressor(**study.best_params, random_state=42)\n",
    "    elif model_name == 'XGBoost':\n",
    "        best_model = XGBRegressor(**study.best_params, random_state=42, n_jobs=-1)\n",
    "    \n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    return best_model, study.best_params, tuning_time, study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60579a",
   "metadata": {},
   "source": [
    "TUNING CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "329ec5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ \n",
      "TUNING CONFIGURATION\n",
      "⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ ⚙️ \n",
      "\n",
      "✅ Hyperparameter Tuning: ENABLED\n",
      "✅ Optimization Method: Optuna (TPE Sampler)\n",
      "✅ Number of Trials: 50\n",
      "✅ Models to tune: Ridge Regression, Lasso Regression, Random Forest, Gradient Boosting, XGBoost\n",
      "\n",
      "✅ Models directories created:\n",
      "   Main: ../models\n",
      "   Tuned: ../models/tuned\n",
      "   Default: ../models/default\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"⚙️ \" * 20)\n",
    "print(\"TUNING CONFIGURATION\")\n",
    "print(\"⚙️ \" * 20)\n",
    "\n",
    "# Configuration\n",
    "ENABLE_TUNING = True  # Set to False to skip tuning\n",
    "N_TRIALS = 50  # Number of Optuna trials\n",
    "\n",
    "print(f\"\\n✅ Hyperparameter Tuning: {'ENABLED' if ENABLE_TUNING else 'DISABLED'}\")\n",
    "print(f\"✅ Optimization Method: Optuna (TPE Sampler)\")\n",
    "print(f\"✅ Number of Trials: {N_TRIALS}\")\n",
    "\n",
    "# Models that will be tuned\n",
    "TUNE_MODELS = ['Ridge_Regression', 'Lasso_Regression', 'Random_Forest', \n",
    "               'Gradient_Boosting', 'XGBoost']\n",
    "\n",
    "print(f\"✅ Models to tune: {', '.join([m.replace('_', ' ') for m in TUNE_MODELS])}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 14: CREATE MODELS DIRECTORIES\n",
    "# ============================================================================\n",
    "models_dir = '../models'\n",
    "tuned_models_dir = f'{models_dir}/tuned'\n",
    "default_models_dir = f'{models_dir}/default'\n",
    "\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(tuned_models_dir, exist_ok=True)\n",
    "os.makedirs(default_models_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\n✅ Models directories created:\")\n",
    "print(f\"   Main: {models_dir}\")\n",
    "print(f\"   Tuned: {tuned_models_dir}\")\n",
    "print(f\"   Default: {default_models_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d2aed",
   "metadata": {},
   "source": [
    "DEFINE AND TUNE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a635215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 16:45:38,771] A new study created in memory with name: no-name-13eb994a-9cbc-409a-8aea-19a67c0d3d9e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 \n",
      "DEFINING AND TUNING MODELS\n",
      "🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 \n",
      "\n",
      "[1/8] Linear Regression (no tuning)\n",
      "\n",
      "[2/8] Ridge Regression\n",
      "  💾 Default model saved: ../models/default/Ridge_Regression_default.joblib\n",
      "\n",
      "======================================================================\n",
      "Tuning Ridge_Regression with Optuna\n",
      "======================================================================\n",
      "  Trials: 50\n",
      "  Metric: MAE (lower is better)\n",
      "  Sampler: TPE (Tree-structured Parzen Estimator)\n",
      "  ✅ Tuning completed in 0.36s\n",
      "  🎯 Best MAE: 1.19\n",
      "  🎯 Best Trial: #50\n",
      "  📋 Best Parameters:\n",
      "      alpha: 0.010065\n",
      "  💾 Tuned model saved: ../models/tuned/Ridge_Regression_tuned.joblib\n",
      "\n",
      "[3/8] Lasso Regression\n",
      "  💾 Default model saved: ../models/default/Lasso_Regression_default.joblib\n",
      "\n",
      "======================================================================\n",
      "Tuning Lasso_Regression with Optuna\n",
      "======================================================================\n",
      "  Trials: 50\n",
      "  Metric: MAE (lower is better)\n",
      "  Sampler: TPE (Tree-structured Parzen Estimator)\n",
      "  ✅ Tuning completed in 53.00s\n",
      "  🎯 Best MAE: 1.19\n",
      "  🎯 Best Trial: #50\n",
      "  📋 Best Parameters:\n",
      "      alpha: 0.010065\n",
      "  💾 Tuned model saved: ../models/tuned/Lasso_Regression_tuned.joblib\n",
      "\n",
      "[4/8] Random Forest\n",
      "  💾 Default model saved: ../models/default/Random_Forest_default.joblib\n",
      "\n",
      "======================================================================\n",
      "Tuning Random_Forest with Optuna\n",
      "======================================================================\n",
      "  Trials: 50\n",
      "  Metric: MAE (lower is better)\n",
      "  Sampler: TPE (Tree-structured Parzen Estimator)\n",
      "  ✅ Tuning completed in 48.03s\n",
      "  🎯 Best MAE: 0.98\n",
      "  🎯 Best Trial: #28\n",
      "  📋 Best Parameters:\n",
      "      n_estimators: 147\n",
      "      max_depth: 28\n",
      "      min_samples_split: 7\n",
      "      min_samples_leaf: 1\n",
      "      max_features: None\n",
      "  💾 Tuned model saved: ../models/tuned/Random_Forest_tuned.joblib\n",
      "\n",
      "[5/8] Gradient Boosting\n",
      "  💾 Default model saved: ../models/default/Gradient_Boosting_default.joblib\n",
      "\n",
      "======================================================================\n",
      "Tuning Gradient_Boosting with Optuna\n",
      "======================================================================\n",
      "  Trials: 50\n",
      "  Metric: MAE (lower is better)\n",
      "  Sampler: TPE (Tree-structured Parzen Estimator)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-10-25 16:53:08,510] Trial 43 failed with parameters: {'n_estimators': 184, 'learning_rate': 0.04977330916622023, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 5, 'subsample': 0.7446021525526079} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Letha\\AppData\\Local\\Temp\\ipykernel_1908\\947159569.py\", line 72, in objective\n",
      "    model.fit(X_train, y_train)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 787, in fit\n",
      "    n_stages = self._fit_stages(\n",
      "        X_train,\n",
      "    ...<8 lines>...\n",
      "        monitor,\n",
      "    )\n",
      "  File \"c:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 883, in _fit_stages\n",
      "    raw_predictions = self._fit_stage(\n",
      "        i,\n",
      "    ...<7 lines>...\n",
      "        X_csr=X_csr,\n",
      "    )\n",
      "  File \"c:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 489, in _fit_stage\n",
      "    tree.fit(\n",
      "    ~~~~~~~~^\n",
      "        X, neg_g_view[:, k], sample_weight=sample_weight, check_input=False\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1404, in fit\n",
      "    super()._fit(\n",
      "    ~~~~~~~~~~~~^\n",
      "        X,\n",
      "        ^^\n",
      "    ...<2 lines>...\n",
      "        check_input=check_input,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 472, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-25 16:53:08,516] Trial 43 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         tuned_model, best_params, tuning_time, study = \u001b[43mtune_with_optuna\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdefault_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m         \u001b[38;5;66;03m# Save tuned model\u001b[39;00m\n\u001b[32m     46\u001b[39m         tuned_path = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtuned_models_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_tuned.joblib\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mtune_with_optuna\u001b[39m\u001b[34m(model_name, model_class, n_trials)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Optimize with progress bar suppression\u001b[39;00m\n\u001b[32m     87\u001b[39m optuna.logging.set_verbosity(optuna.logging.WARNING)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m tuning_time = time.time() - start_time\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ✅ Tuning completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtuning_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mtune_with_optuna.<locals>.objective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     69\u001b[39m     model = XGBRegressor(**params)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m y_pred = model.predict(X_test)\n\u001b[32m     74\u001b[39m mae = mean_absolute_error(y_test, y_pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001b[39m, in \u001b[36mBaseGradientBoosting.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, monitor)\u001b[39m\n\u001b[32m    784\u001b[39m     \u001b[38;5;28mself\u001b[39m._resize_state()\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m n_stages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_stages != \u001b[38;5;28mself\u001b[39m.estimators_.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stages\u001b[39m\u001b[34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[39m\n\u001b[32m    876\u001b[39m         initial_loss = factor * \u001b[38;5;28mself\u001b[39m._loss(\n\u001b[32m    877\u001b[39m             y_true=y_oob_masked,\n\u001b[32m    878\u001b[39m             raw_prediction=raw_predictions[~sample_mask],\n\u001b[32m    879\u001b[39m             sample_weight=sample_weight_oob_masked,\n\u001b[32m    880\u001b[39m         )\n\u001b[32m    882\u001b[39m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m raw_predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:489\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stage\u001b[39m\u001b[34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[39m\n\u001b[32m    486\u001b[39m     sample_weight = sample_weight * sample_mask.astype(np.float64)\n\u001b[32m    488\u001b[39m X = X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[32m    494\u001b[39m X_for_tree_update = X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001b[39m, in \u001b[36mDecisionTreeRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1376\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[32m   1377\u001b[39m \n\u001b[32m   1378\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1401\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"🎯 \" * 20)\n",
    "print(\"DEFINING AND TUNING MODELS\")\n",
    "print(\"🎯 \" * 20)\n",
    "\n",
    "models = {}\n",
    "tuning_results = {}\n",
    "\n",
    "# 1. Linear Regression (no tuning needed)\n",
    "print(\"\\n[1/8] Linear Regression (no tuning)\")\n",
    "models['Linear_Regression'] = LinearRegression()\n",
    "\n",
    "# 2-6. Models with optional tuning\n",
    "tunable_models = {\n",
    "    'Ridge_Regression': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso_Regression': Lasso(alpha=1.0, random_state=42),\n",
    "    'Random_Forest': RandomForestRegressor(\n",
    "        n_estimators=100, max_depth=20, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'Gradient_Boosting': GradientBoostingRegressor(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42\n",
    "    ),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=7, random_state=42, n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "for i, (name, default_model) in enumerate(tunable_models.items(), 2):\n",
    "    print(f\"\\n[{i}/8] {name.replace('_', ' ')}\")\n",
    "    \n",
    "    # Save default model\n",
    "    default_path = f'{default_models_dir}/{name}_default.joblib'\n",
    "    joblib.dump(default_model, default_path, compress=3)\n",
    "    print(f\"  💾 Default model saved: {default_path}\")\n",
    "    \n",
    "    if not ENABLE_TUNING or name not in TUNE_MODELS:\n",
    "        models[name] = default_model\n",
    "        print(f\"  ⚠️  Using default parameters (tuning disabled)\")\n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "            tuned_model, best_params, tuning_time, study = tune_with_optuna(\n",
    "                name, type(default_model), n_trials=N_TRIALS\n",
    "            )\n",
    "            \n",
    "            # Save tuned model\n",
    "            tuned_path = f'{tuned_models_dir}/{name}_tuned.joblib'\n",
    "            joblib.dump(tuned_model, tuned_path, compress=3)\n",
    "            print(f\"  💾 Tuned model saved: {tuned_path}\")\n",
    "            \n",
    "            models[name] = tuned_model\n",
    "            tuning_results[name] = {\n",
    "                'method': 'Optuna',\n",
    "                'best_params': best_params,\n",
    "                'tuning_time': tuning_time,\n",
    "                'best_value': study.best_value,\n",
    "                'n_trials': len(study.trials)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Tuning failed: {e}\")\n",
    "            models[name] = default_model\n",
    "\n",
    "# 7. Voting Regressor (uses tuned models if available)\n",
    "print(f\"\\n[7/8] Voting Regressor\")\n",
    "print(f\"  📦 Ensemble of: Ridge, RF, GB, XGB\")\n",
    "models['Voting_Regressor'] = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('ridge', models['Ridge_Regression']),\n",
    "        ('rf', models['Random_Forest']),\n",
    "        ('gb', models['Gradient_Boosting']),\n",
    "        ('xgb', models['XGBoost'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 8. Stacking Regressor (uses tuned models if available)\n",
    "print(f\"\\n[8/8] Stacking Regressor\")\n",
    "print(f\"  📦 Base: Ridge, RF, GB, XGB | Final: Ridge\")\n",
    "models['Stacking_Regressor'] = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('ridge', models['Ridge_Regression']),\n",
    "        ('rf', models['Random_Forest']),\n",
    "        ('gb', models['Gradient_Boosting']),\n",
    "        ('xgb', models['XGBoost'])\n",
    "    ],\n",
    "    final_estimator=Ridge(alpha=1.0, random_state=42),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ All {len(models)} models defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad00ac",
   "metadata": {},
   "source": [
    "TRAIN AND EVALUATE ALL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c41311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 \n",
      "TRAINING ALL MODELS\n",
      "🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 \n",
      "\n",
      "Training data: 17,772 samples × 26 features\n",
      "Test data:     4,449 samples × 26 features\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[1/8] Training: Linear Regression\n",
      "======================================================================\n",
      "  ⏳ Fitting model...\n",
      "  ✅ Training completed in 0.01s\n",
      "  🔮 Making predictions...\n",
      "  📊 Train R²: 0.4917 | Test R²: 0.4020\n",
      "  📊 Train MAE: 1 | Test MAE: 1\n",
      "  📊 Overfitting: 0.0897 (⚠️  Slight)\n",
      "  💾 Saving final model...\n",
      "  ✅ Saved: ../models/Linear_Regression.joblib\n",
      "\n",
      "======================================================================\n",
      "[2/8] Training: Ridge Regression\n",
      "======================================================================\n",
      "  ⏳ Fitting model...\n",
      "  ✅ Training completed in 0.00s\n",
      "  🔮 Making predictions...\n",
      "  📊 Train R²: 0.4917 | Test R²: 0.4020\n",
      "  📊 Train MAE: 1 | Test MAE: 1\n",
      "  📊 Overfitting: 0.0897 (⚠️  Slight)\n",
      "  💾 Saving final model...\n",
      "  ✅ Saved: ../models/Ridge_Regression.joblib\n",
      "  ✅ Also saved in tuned directory: ../models/tuned/Ridge_Regression_trained.joblib\n",
      "\n",
      "======================================================================\n",
      "[3/8] Training: Lasso Regression\n",
      "======================================================================\n",
      "  ⏳ Fitting model...\n",
      "  ✅ Training completed in 1.43s\n",
      "  🔮 Making predictions...\n",
      "  📊 Train R²: 0.4904 | Test R²: 0.4115\n",
      "  📊 Train MAE: 1 | Test MAE: 1\n",
      "  📊 Overfitting: 0.0789 (⚠️  Slight)\n",
      "  💾 Saving final model...\n",
      "  ✅ Saved: ../models/Lasso_Regression.joblib\n",
      "  ✅ Also saved in tuned directory: ../models/tuned/Lasso_Regression_trained.joblib\n",
      "\n",
      "======================================================================\n",
      "[4/8] Training: Random Forest\n",
      "======================================================================\n",
      "  ⏳ Fitting model...\n",
      "  ✅ Training completed in 1.29s\n",
      "  🔮 Making predictions...\n",
      "  📊 Train R²: 0.8898 | Test R²: 0.5981\n",
      "  📊 Train MAE: 1 | Test MAE: 1\n",
      "  📊 Overfitting: 0.2917 (❌ High)\n",
      "  💾 Saving final model...\n",
      "  ✅ Saved: ../models/Random_Forest.joblib\n",
      "  ✅ Also saved in tuned directory: ../models/tuned/Random_Forest_trained.joblib\n",
      "\n",
      "======================================================================\n",
      "[5/8] Training: Gradient Boosting\n",
      "======================================================================\n",
      "  ⏳ Fitting model...\n",
      "  ✅ Training completed in 8.06s\n",
      "  🔮 Making predictions...\n",
      "  📊 Train R²: 0.8651 | Test R²: 0.6151\n",
      "  📊 Train MAE: 1 | Test MAE: 1\n",
      "  📊 Overfitting: 0.2500 (❌ High)\n",
      "  💾 Saving final model...\n",
      "  ✅ Saved: ../models/Gradient_Boosting.joblib\n",
      "  ✅ Also saved in tuned directory: ../models/tuned/Gradient_Boosting_trained.joblib\n",
      "\n",
      "======================================================================\n",
      "[6/8] Training: XGBoost\n",
      "======================================================================\n",
      "  ⏳ Fitting model...\n",
      "  ✅ Training completed in 0.31s\n",
      "  🔮 Making predictions...\n",
      "  📊 Train R²: 0.8018 | Test R²: 0.6165\n",
      "  📊 Train MAE: 1 | Test MAE: 1\n",
      "  📊 Overfitting: 0.1853 (❌ High)\n",
      "  💾 Saving final model...\n",
      "  ✅ Saved: ../models/XGBoost.joblib\n",
      "  ✅ Also saved in tuned directory: ../models/tuned/XGBoost_trained.joblib\n",
      "\n",
      "======================================================================\n",
      "[7/8] Training: Voting Regressor\n",
      "======================================================================\n",
      "  ⏳ Fitting model...\n",
      "  ✅ Training completed in 9.51s\n",
      "  🔮 Making predictions...\n",
      "  📊 Train R²: 0.8070 | Test R²: 0.6035\n",
      "  📊 Train MAE: 1 | Test MAE: 1\n",
      "  📊 Overfitting: 0.2035 (❌ High)\n",
      "  💾 Saving final model...\n",
      "  ✅ Saved: ../models/Voting_Regressor.joblib\n",
      "\n",
      "======================================================================\n",
      "[8/8] Training: Stacking Regressor\n",
      "======================================================================\n",
      "  ⏳ Fitting model...\n",
      "  ✅ Training completed in 48.45s\n",
      "  🔮 Making predictions...\n",
      "  📊 Train R²: 0.8303 | Test R²: 0.6188\n",
      "  📊 Train MAE: 1 | Test MAE: 1\n",
      "  📊 Overfitting: 0.2115 (❌ High)\n",
      "  💾 Saving final model...\n",
      "  ✅ Saved: ../models/Stacking_Regressor.joblib\n",
      "\n",
      "✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ \n",
      "ALL MODELS TRAINED & SAVED!\n",
      "✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"🚀 \" * 20)\n",
    "print(\"TRAINING ALL MODELS\")\n",
    "print(\"🚀 \" * 20)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "print(f\"\\nTraining data: {X_train.shape[0]:,} samples × {X_train.shape[1]} features\")\n",
    "print(f\"Test data:     {X_test.shape[0]:,} samples × {X_test.shape[1]} features\\n\")\n",
    "\n",
    "# Train each model\n",
    "for i, (name, model) in enumerate(models.items(), 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{i}/{len(models)}] Training: {name.replace('_', ' ')}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train model\n",
    "        print(\"  ⏳ Fitting model...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Training time\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"  ✅ Training completed in {training_time:.2f}s\")\n",
    "        \n",
    "        # Predict\n",
    "        print(\"  🔮 Making predictions...\")\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate on train set\n",
    "        train_metrics = evaluate_model(y_train, y_train_pred, f\"{name} (Train)\")\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_metrics = evaluate_model(y_test, y_test_pred, f\"{name} (Test)\")\n",
    "        \n",
    "        # Print quick summary\n",
    "        print(f\"  📊 Train R²: {train_metrics['R²']:.4f} | Test R²: {test_metrics['R²']:.4f}\")\n",
    "        print(f\"  📊 Train MAE: {train_metrics['MAE']:,.0f} | Test MAE: {test_metrics['MAE']:,.0f}\")\n",
    "        \n",
    "        # Calculate overfitting\n",
    "        overfit = train_metrics['R²'] - test_metrics['R²']\n",
    "        overfit_status = \"✅ Good\" if overfit < 0.05 else \"⚠️  Slight\" if overfit < 0.1 else \"❌ High\"\n",
    "        print(f\"  📊 Overfitting: {overfit:.4f} ({overfit_status})\")\n",
    "        \n",
    "        # Add tuning info if available\n",
    "        tuned = \"✅ Yes\" if name in tuning_results else \"❌ No\"\n",
    "        tuning_time_total = tuning_results[name]['tuning_time'] if name in tuning_results else 0\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Model': name.replace('_', ' '),\n",
    "            'Tuned': tuned,\n",
    "            'Train_R²': train_metrics['R²'],\n",
    "            'Test_R²': test_metrics['R²'],\n",
    "            'Train_MAE': train_metrics['MAE'],\n",
    "            'Test_MAE': test_metrics['MAE'],\n",
    "            'Train_RMSE': train_metrics['RMSE'],\n",
    "            'Test_RMSE': test_metrics['RMSE'],\n",
    "            'MAPE (%)': test_metrics['MAPE (%)'],\n",
    "            'Adj_R²': test_metrics['Adj_R²'],\n",
    "            'Training_Time (s)': training_time,\n",
    "            'Tuning_Time (s)': tuning_time_total,\n",
    "            'Total_Time (s)': training_time + tuning_time_total,\n",
    "            'Overfit': overfit\n",
    "        })\n",
    "        \n",
    "        # Save model (3 locations)\n",
    "        # 1. Main directory (final trained model)\n",
    "        model_path = f'{models_dir}/{name}.joblib'\n",
    "        print(f\"  💾 Saving final model...\")\n",
    "        joblib.dump(model, model_path, compress=3)\n",
    "        print(f\"  ✅ Saved: {model_path}\")\n",
    "        \n",
    "        # 2. If this is a tuned model, also save in tuned directory\n",
    "        if name in tuning_results:\n",
    "            tuned_trained_path = f'{tuned_models_dir}/{name}_trained.joblib'\n",
    "            joblib.dump(model, tuned_trained_path, compress=3)\n",
    "            print(f\"  ✅ Also saved in tuned directory: {tuned_trained_path}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Store in memory for later use\n",
    "        trained_models[name] = model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error training {name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"✅ \" * 20)\n",
    "print(\"ALL MODELS TRAINED & SAVED!\")\n",
    "print(\"✅ \" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8538aa3",
   "metadata": {},
   "source": [
    "COMPARE ALL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbc2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 \n",
      "MODEL COMPARISON\n",
      "📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 📊 \n",
      "\n",
      "========================================================================================================================\n",
      "PERFORMANCE RANKING (sorted by Test R²)\n",
      "========================================================================================================================\n",
      "             Model Tuned  Train_R²  Test_R²  Train_MAE  Test_MAE  Train_RMSE  Test_RMSE  MAPE (%)   Adj_R²  Training_Time (s)  Tuning_Time (s)  Total_Time (s)  Overfit\n",
      "Stacking Regressor  ❌ No  0.830283 0.618791   0.640922  0.967225    0.847923   1.285388 18.567903 0.616550          48.454065         0.000000       48.454065 0.211491\n",
      "           XGBoost ✅ Yes  0.801791 0.616517   0.692654  0.972739    0.916336   1.289217 18.664387 0.614262           0.310204        14.333030       14.643234 0.185274\n",
      " Gradient Boosting ✅ Yes  0.865069 0.615061   0.557159  0.966888    0.756048   1.291663 18.488343 0.612797           8.057933       291.990098      300.048032 0.250008\n",
      "  Voting Regressor  ❌ No  0.806988 0.603537   0.697458  0.990823    0.904245   1.310854 19.158447 0.601206           9.509927         0.000000        9.509927 0.203450\n",
      "     Random Forest ✅ Yes  0.889787 0.598063   0.510894  0.984423    0.683297   1.319872 19.046585 0.595700           1.292047        45.033384       46.325431 0.291724\n",
      "  Lasso Regression ✅ Yes  0.490358 0.411506   1.153600  1.190271    1.469353   1.597070 23.254591 0.408046           1.434911        42.437599       43.872510 0.078852\n",
      "  Ridge Regression ✅ Yes  0.491745 0.402022   1.149856  1.189059    1.467352   1.609888 23.127087 0.398506           0.004818         0.415205        0.420022 0.089724\n",
      " Linear Regression  ❌ No  0.491745 0.402021   1.149856  1.189059    1.467352   1.609889 23.127085 0.398505           0.010867         0.000000        0.010867 0.089724\n",
      "\n",
      "✅ Comparison table saved: ../models/model_comparison.csv\n",
      "\n",
      "🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 \n",
      "OPTUNA HYPERPARAMETER TUNING SUMMARY\n",
      "🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 🔬 \n",
      "\n",
      "======================================================================\n",
      "Model: Ridge Regression\n",
      "======================================================================\n",
      "  Optimization Method: Optuna\n",
      "  Number of Trials: 50\n",
      "  Tuning Time: 0.42s\n",
      "  Best MAE: 1.19\n",
      "  Best Parameters:\n",
      "    alpha: 0.010065\n",
      "\n",
      "======================================================================\n",
      "Model: Lasso Regression\n",
      "======================================================================\n",
      "  Optimization Method: Optuna\n",
      "  Number of Trials: 50\n",
      "  Tuning Time: 42.44s\n",
      "  Best MAE: 1.19\n",
      "  Best Parameters:\n",
      "    alpha: 0.010065\n",
      "\n",
      "======================================================================\n",
      "Model: Random Forest\n",
      "======================================================================\n",
      "  Optimization Method: Optuna\n",
      "  Number of Trials: 50\n",
      "  Tuning Time: 45.03s\n",
      "  Best MAE: 0.98\n",
      "  Best Parameters:\n",
      "    n_estimators: 147\n",
      "    max_depth: 28\n",
      "    min_samples_split: 7\n",
      "    min_samples_leaf: 1\n",
      "    max_features: None\n",
      "\n",
      "======================================================================\n",
      "Model: Gradient Boosting\n",
      "======================================================================\n",
      "  Optimization Method: Optuna\n",
      "  Number of Trials: 50\n",
      "  Tuning Time: 291.99s\n",
      "  Best MAE: 0.97\n",
      "  Best Parameters:\n",
      "    n_estimators: 170\n",
      "    learning_rate: 0.059588\n",
      "    max_depth: 10\n",
      "    min_samples_split: 2\n",
      "    min_samples_leaf: 5\n",
      "    subsample: 0.765823\n",
      "\n",
      "======================================================================\n",
      "Model: XGBoost\n",
      "======================================================================\n",
      "  Optimization Method: Optuna\n",
      "  Number of Trials: 50\n",
      "  Tuning Time: 14.33s\n",
      "  Best MAE: 0.97\n",
      "  Best Parameters:\n",
      "    n_estimators: 174\n",
      "    learning_rate: 0.050769\n",
      "    max_depth: 9\n",
      "    min_child_weight: 5\n",
      "    subsample: 0.771644\n",
      "    colsample_bytree: 0.812754\n",
      "    gamma: 0.379257\n",
      "    reg_alpha: 0.749893\n",
      "    reg_lambda: 0.782680\n",
      "\n",
      "✅ Optuna tuning results saved: ../models/optuna_tuning_results.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"📊 \" * 20)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"📊 \" * 20)\n",
    "\n",
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by Test R² (descending)\n",
    "results_df = results_df.sort_values('Test_R²', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"PERFORMANCE RANKING (sorted by Test R²)\")\n",
    "print(\"=\"*120)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save comparison table\n",
    "comparison_path = f'{models_dir}/model_comparison.csv'\n",
    "results_df.to_csv(comparison_path, index=False)\n",
    "print(f\"\\n✅ Comparison table saved: {comparison_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 18: TUNING RESULTS SUMMARY\n",
    "# ============================================================================\n",
    "if tuning_results:\n",
    "    print(\"\\n\" + \"🔬 \" * 20)\n",
    "    print(\"OPTUNA HYPERPARAMETER TUNING SUMMARY\")\n",
    "    print(\"🔬 \" * 20)\n",
    "    \n",
    "    for model_name, info in tuning_results.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Model: {model_name.replace('_', ' ')}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"  Optimization Method: {info['method']}\")\n",
    "        print(f\"  Number of Trials: {info['n_trials']}\")\n",
    "        print(f\"  Tuning Time: {info['tuning_time']:.2f}s\")\n",
    "        print(f\"  Best MAE: {info['best_value']:,.2f}\")\n",
    "        print(f\"  Best Parameters:\")\n",
    "        for param, value in info['best_params'].items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"    {param}: {value:.6f}\")\n",
    "            else:\n",
    "                print(f\"    {param}: {value}\")\n",
    "    \n",
    "    # Save tuning results\n",
    "    tuning_path = f'{models_dir}/optuna_tuning_results.json'\n",
    "    with open(tuning_path, 'w') as f:\n",
    "        # Convert to JSON-serializable format\n",
    "        tuning_json = {}\n",
    "        for k, v in tuning_results.items():\n",
    "            tuning_json[k] = {\n",
    "                'method': v['method'],\n",
    "                'n_trials': int(v['n_trials']),\n",
    "                'tuning_time': float(v['tuning_time']),\n",
    "                'best_value': float(v['best_value']),\n",
    "                'best_params': {pk: float(pv) if isinstance(pv, (np.integer, np.floating)) else pv \n",
    "                               for pk, pv in v['best_params'].items()}\n",
    "            }\n",
    "        json.dump(tuning_json, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✅ Optuna tuning results saved: {tuning_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361a6eb",
   "metadata": {},
   "source": [
    "SUMMARY STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454b251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SUMMARY STATISTICS\n",
      "======================================================================\n",
      "\n",
      "🏆 Best Model (Test R²):\n",
      "   Stacking Regressor\n",
      "   Tuned: ❌ No\n",
      "   R² = 0.6188\n",
      "   MAE = 0.97\n",
      "   RMSE = 1.29\n",
      "\n",
      "🎯 Best Model (Test MAE):\n",
      "   Gradient Boosting\n",
      "   Tuned: ✅ Yes\n",
      "   MAE = 0.97\n",
      "   R² = 0.6151\n",
      "\n",
      "⚡ Fastest Model:\n",
      "   Linear Regression\n",
      "   Total Time = 0.01s\n",
      "   R² = 0.4020\n",
      "\n",
      "📊 Average Metrics:\n",
      "   Test R²:       0.5334\n",
      "   Test MAE:      1.06\n",
      "   Test RMSE:     1.41\n",
      "   Training Time: 8.63s\n",
      "   Tuning Time:   49.28s\n",
      "   Total Time:    57.91s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_r2_idx = results_df['Test_R²'].idxmax()\n",
    "best_mae_idx = results_df['Test_MAE'].idxmin()\n",
    "fastest_idx = results_df['Total_Time (s)'].idxmin()\n",
    "\n",
    "print(f\"\\n🏆 Best Model (Test R²):\")\n",
    "print(f\"   {results_df.loc[best_r2_idx, 'Model']}\")\n",
    "print(f\"   Tuned: {results_df.loc[best_r2_idx, 'Tuned']}\")\n",
    "print(f\"   R² = {results_df.loc[best_r2_idx, 'Test_R²']:.4f}\")\n",
    "print(f\"   MAE = {results_df.loc[best_r2_idx, 'Test_MAE']:,.2f}\")\n",
    "print(f\"   RMSE = {results_df.loc[best_r2_idx, 'Test_RMSE']:,.2f}\")\n",
    "\n",
    "print(f\"\\n🎯 Best Model (Test MAE):\")\n",
    "print(f\"   {results_df.loc[best_mae_idx, 'Model']}\")\n",
    "print(f\"   Tuned: {results_df.loc[best_mae_idx, 'Tuned']}\")\n",
    "print(f\"   MAE = {results_df.loc[best_mae_idx, 'Test_MAE']:,.2f}\")\n",
    "print(f\"   R² = {results_df.loc[best_mae_idx, 'Test_R²']:.4f}\")\n",
    "\n",
    "print(f\"\\n⚡ Fastest Model:\")\n",
    "print(f\"   {results_df.loc[fastest_idx, 'Model']}\")\n",
    "print(f\"   Total Time = {results_df.loc[fastest_idx, 'Total_Time (s)']:.2f}s\")\n",
    "print(f\"   R² = {results_df.loc[fastest_idx, 'Test_R²']:.4f}\")\n",
    "\n",
    "print(f\"\\n📊 Average Metrics:\")\n",
    "print(f\"   Test R²:       {results_df['Test_R²'].mean():.4f}\")\n",
    "print(f\"   Test MAE:      {results_df['Test_MAE'].mean():,.2f}\")\n",
    "print(f\"   Test RMSE:     {results_df['Test_RMSE'].mean():,.2f}\")\n",
    "print(f\"   Training Time: {results_df['Training_Time (s)'].mean():.2f}s\")\n",
    "print(f\"   Tuning Time:   {results_df['Tuning_Time (s)'].mean():.2f}s\")\n",
    "print(f\"   Total Time:    {results_df['Total_Time (s)'].mean():.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd807007",
   "metadata": {},
   "source": [
    "DETAILED BEST MODEL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec71883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 \n",
      "BEST MODEL DETAILED ANALYSIS\n",
      "🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 🏆 \n",
      "\n",
      "🥇 Best Model: Stacking Regressor\n",
      "\n",
      "======================================================================\n",
      "📊 Stacking Regressor (Train) Performance\n",
      "======================================================================\n",
      "  MAE (Mean Absolute Error):                   0.64\n",
      "  RMSE (Root Mean Squared Error):              0.85\n",
      "  R² Score:                                  0.8303\n",
      "  Adjusted R²:                               0.8300\n",
      "  MAPE (Mean Abs Percentage Error):           11.93%\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "📊 Stacking Regressor (Test) Performance\n",
      "======================================================================\n",
      "  MAE (Mean Absolute Error):                   0.97\n",
      "  RMSE (Root Mean Squared Error):              1.29\n",
      "  R² Score:                                  0.6188\n",
      "  Adjusted R²:                               0.6166\n",
      "  MAPE (Mean Abs Percentage Error):           18.57%\n",
      "======================================================================\n",
      "\n",
      "💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 \n",
      "SAVING METADATA\n",
      "💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 💾 \n",
      "✅ Metadata saved: ../models/best_model_metadata.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"🏆 \" * 20)\n",
    "print(\"BEST MODEL DETAILED ANALYSIS\")\n",
    "print(\"🏆 \" * 20)\n",
    "\n",
    "# Get best model (by Test R²)\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model_key = best_model_name.replace(' ', '_')\n",
    "best_model = trained_models[best_model_key]\n",
    "\n",
    "print(f\"\\n🥇 Best Model: {best_model_name}\")\n",
    "if best_model_key in tuning_results:\n",
    "    print(f\"   ✅ Hyperparameter Tuning: Optuna\")\n",
    "    print(f\"   ✅ Trials: {tuning_results[best_model_key]['n_trials']}\")\n",
    "    print(f\"   ✅ Tuning Time: {tuning_results[best_model_key]['tuning_time']:.2f}s\")\n",
    "\n",
    "# Detailed metrics\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "train_metrics = evaluate_model(y_train, y_train_pred, f\"{best_model_name} (Train)\")\n",
    "test_metrics = evaluate_model(y_test, y_test_pred, f\"{best_model_name} (Test)\")\n",
    "\n",
    "print_metrics(train_metrics)\n",
    "print_metrics(test_metrics)\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TOP 15 MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(feature_importance.head(15).to_string(index=False))\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance.to_csv(f'{models_dir}/feature_importance_{best_model_key}.csv', index=False)\n",
    "    print(f\"\\n✅ Feature importance saved: {models_dir}/feature_importance_{best_model_key}.csv\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL 21: SAVE BEST MODEL METADATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"💾 \" * 20)\n",
    "print(\"SAVING METADATA\")\n",
    "print(\"💾 \" * 20)\n",
    "\n",
    "best_row = results_df.iloc[0]\n",
    "\n",
    "metadata = {\n",
    "    'best_model_name': best_model_name,\n",
    "    'best_model_file': f'{best_model_key}.joblib',\n",
    "    'hyperparameter_tuning': {\n",
    "        'method': 'Optuna (TPE Sampler)',\n",
    "        'tuned': best_model_key in tuning_results,\n",
    "        'n_trials': tuning_results[best_model_key]['n_trials'] if best_model_key in tuning_results else 0,\n",
    "        'best_params': tuning_results[best_model_key]['best_params'] if best_model_key in tuning_results else {}\n",
    "    },\n",
    "    'test_r2': float(best_row['Test_R²']),\n",
    "    'test_mae': float(best_row['Test_MAE']),\n",
    "    'test_rmse': float(best_row['Test_RMSE']),\n",
    "    'mape': float(best_row['MAPE (%)']),\n",
    "    'adj_r2': float(best_row['Adj_R²']),\n",
    "    'train_r2': float(best_row['Train_R²']),\n",
    "    'overfitting': float(best_row['Overfit']),\n",
    "    'training_time': float(best_row['Training_Time (s)']),\n",
    "    'tuning_time': float(best_row['Tuning_Time (s)']),\n",
    "    'total_time': float(best_row['Total_Time (s)']),\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'n_features': int(X_train.shape[1]),\n",
    "    'n_train_samples': int(X_train.shape[0]),\n",
    "    'n_test_samples': int(X_test.shape[0]),\n",
    "    'feature_names': list(X_train.columns),\n",
    "    'all_models_trained': [m.replace('_', ' ') for m in trained_models.keys()]\n",
    "}\n",
    "\n",
    "metadata_path = f'{models_dir}/best_model_metadata.json'\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Metadata saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "770619a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 \n",
      "PREPROCESSING PIPELINE - TEST/NEW DATA\n",
      "🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 \n",
      "✓ Index reset for safe processing\n",
      "\n",
      "============================================================\n",
      "STEP 1: STRING NORMALIZATION\n",
      "============================================================\n",
      "City unique values: 1\n",
      "District unique values: 1\n",
      "\n",
      "============================================================\n",
      "STEP 2: REMOVE LEAKAGE FEATURES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STEP 3: HANDLE MISSING VALUES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STEP 6: FEATURE ENGINEERING\n",
      "============================================================\n",
      "✓ Created 13 new features\n",
      "\n",
      "============================================================\n",
      "STEP 7: ENCODE CATEGORICAL FEATURES\n",
      "============================================================\n",
      "  ✓ District: Applied stored encoding (test)\n",
      "  ✓ City: Applied stored encoding (test)\n",
      "  ✓ Legal status: Applied stored encoding (test)\n",
      "  ✓ Furniture state: Applied stored encoding (test)\n",
      "  ✓ Location_Tier: Applied stored encoding (test)\n",
      "\n",
      "============================================================\n",
      "STEP 8: SCALE NUMERIC FEATURES\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Transform\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m X_test = \u001b[43mpreprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Transform successful!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 321\u001b[39m, in \u001b[36mRealEstatePreprocessor.transform\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# Drop all non-feature columns\u001b[39;00m\n\u001b[32m    317\u001b[39m X = df.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mPrice\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAddress\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCity\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDistrict\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    318\u001b[39m                      \u001b[33m'\u001b[39m\u001b[33mLegal status\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFurniture state\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLocation_Tier\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    319\u001b[39m                      \u001b[33m'\u001b[39m\u001b[33mHouse direction\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBalcony direction\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m    320\u001b[39m             errors=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m✅ \u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m20\u001b[39m)\n\u001b[32m    324\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFINAL SHAPE: X=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 255\u001b[39m, in \u001b[36mRealEstatePreprocessor.scale_features\u001b[39m\u001b[34m(self, df, is_train)\u001b[39m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Fitted scaler on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(numeric_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features (TRAIN)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m     df[numeric_cols] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Applied scaler to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(numeric_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features (TEST)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1075\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1072\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1074\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1087\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Letha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names must be in the same order as they were in fit.\n"
     ]
    }
   ],
   "source": [
    "# Tạo dữ liệu test giả\n",
    "test_input = pd.DataFrame([{\n",
    "    'Address': 'Test Address, Quận 1, Hồ Chí Minh',\n",
    "    'Area': 100.0,\n",
    "    'Frontage': 5.0,\n",
    "    'Access Road': 4.0,\n",
    "    'Floors': 3.0,\n",
    "    'Bedrooms': 3.0,\n",
    "    'Bathrooms': 2.0,\n",
    "    'Legal status': 'Have Certificate',\n",
    "    'Furniture state': 'Full',\n",
    "    'Price': 0.0,\n",
    "    'City': 'Hồ Chí Minh',\n",
    "    'District': 'Quận 1',\n",
    "    'House direction': 'East',\n",
    "    'Balcony direction': 'South',\n",
    "    'Has_Frontage': 1,\n",
    "    'Has_Access_Road': 1,\n",
    "    'Has_House_Direction': 1,\n",
    "    'Has_Balcony_Direction': 1\n",
    "}])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Transform\n",
    "X_test = preprocessor.transform(test_input)\n",
    "\n",
    "print(f\"\\n✅ Transform successful!\")\n",
    "print(f\"   Shape: {X_test.shape}\")\n",
    "print(f\"   Columns: {list(X_test.columns)}\")\n",
    "print(f\"   Feature count: {len(X_test.columns)}\")\n",
    "print(f\"   Expected: {len(metadata['feature_names'])}\")\n",
    "\n",
    "if list(X_test.columns) == metadata['feature_names']:\n",
    "    print(\"✅ Feature order PERFECT!\")\n",
    "else:\n",
    "    print(\"⚠️  Feature order mismatch!\")\n",
    "\n",
    "# Load model và predict\n",
    "model = joblib.load('../models/Stacking_Regressor.joblib')\n",
    "prediction = model.predict(X_test.to_numpy())[0]\n",
    "\n",
    "print(f\"\\n✅ Prediction successful: {prediction:.2f} tỷ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
