{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae72210",
   "metadata": {},
   "source": [
    "COMPLETE DATA PROCESSING PIPELINE - VIETNAM REAL ESTATE\n",
    "Fixed: All variables defined, proper flow, no missing references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d9416c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce1715",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 1. LOAD CLEANED DATA\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d55e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] LOADING DATA\n",
      "================================================================================\n",
      "✓ Loaded: (22245, 13)\n",
      "\n",
      "Initial missing values:\n",
      "                   Count  Percentage (%)\n",
      "Balcony direction  19432       87.354462\n",
      "House direction    17411       78.269274\n",
      "Access Road        10634       47.804001\n",
      "Furniture state     9590       43.110811\n",
      "Frontage            9076       40.800180\n",
      "Bathrooms           5252       23.609800\n",
      "Bedrooms            3606       16.210384\n",
      "Legal status        3068       13.791863\n",
      "Floors              2093        9.408856\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n[1] LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "df = pd.read_csv('../data/housing_data_cleaned.csv')\n",
    "df_processed = df.copy()\n",
    "print(f\"✓ Loaded: {df_processed.shape}\")\n",
    "print(\"\\nInitial missing values:\")\n",
    "missing_info = pd.DataFrame({\n",
    "    'Count': df_processed.isnull().sum(),\n",
    "    'Percentage (%)': (df_processed.isnull().sum() / len(df_processed)) * 100\n",
    "})\n",
    "missing_info = missing_info[missing_info['Count'] > 0].sort_values(\n",
    "    by='Percentage (%)', ascending=False\n",
    ")\n",
    "if not missing_info.empty:\n",
    "    print(missing_info)\n",
    "else:\n",
    "    print(\"No missing values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95acc2",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 2. EXTRACT LOCATION FEATURES FROM ADDRESS\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7ebb2f5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2] EXTRACTING LOCATION FROM ADDRESS\n",
      "================================================================================\n",
      "✓ Created 3 location columns:\n",
      "  - Ward: 22245 values\n",
      "  - District: 22245 values\n",
      "  - City: 22245 values\n",
      "\n",
      "Sample extraction (first 3):\n",
      "\n",
      "Address: Đường Nguyễn Văn Khối, Phường 11, Gò Vấp, Hồ Chí Minh\n",
      "  → Ward: Phường 11\n",
      "  → District: Gò Vấp\n",
      "  → City: Hồ Chí Minh\n",
      "\n",
      "Address: Đường Quang Trung, Phường 8, Gò Vấp, Hồ Chí Minh\n",
      "  → Ward: Phường 8\n",
      "  → District: Gò Vấp\n",
      "  → City: Hồ Chí Minh\n",
      "\n",
      "Address: Dự án Him Lam Thường Tín, Huyện Thường Tín, Hà Nội\n",
      "  → Ward: Dự án Him Lam Thường Tín\n",
      "  → District: Huyện Thường Tín\n",
      "  → City: Hà Nội\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n[2] EXTRACTING LOCATION FROM ADDRESS\")\n",
    "print(\"=\"*80)\n",
    "def extract_location_info(address):\n",
    "    \"\"\"Extract Ward, District, City from Address (last 3 parts)\"\"\"\n",
    "    if pd.isna(address):\n",
    "        return pd.Series({\n",
    "            'Ward': None,\n",
    "            'District': None,\n",
    "            'City': None\n",
    "        })\n",
    "    \n",
    "    parts = [p.strip() for p in str(address).split(',')]\n",
    "    \n",
    "    ward = parts[-3] if len(parts) >= 3 else None\n",
    "    district = parts[-2] if len(parts) >= 2 else None\n",
    "    city = parts[-1] if len(parts) >= 1 else None\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Ward': ward,\n",
    "        'District': district,\n",
    "        'City': city\n",
    "    })\n",
    "# Extract location info\n",
    "location_info = df_processed['Address'].apply(extract_location_info)\n",
    "\n",
    "# Drop existing columns if they exist (avoid duplicates)\n",
    "for col in ['Ward', 'District', 'City']:\n",
    "    if col in df_processed.columns:\n",
    "        df_processed = df_processed.drop(columns=[col])\n",
    "# Add location columns\n",
    "df_processed = pd.concat([df_processed, location_info], axis=1)\n",
    "\n",
    "print(\"✓ Created 3 location columns:\")\n",
    "print(f\"  - Ward: {df_processed['Ward'].notna().sum()} values\")\n",
    "print(f\"  - District: {df_processed['District'].notna().sum()} values\")\n",
    "print(f\"  - City: {df_processed['City'].notna().sum()} values\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample extraction (first 3):\")\n",
    "for idx, row in df_processed[['Address', 'Ward', 'District', 'City']].head(3).iterrows():\n",
    "    print(f\"\\nAddress: {row['Address']}\")\n",
    "    print(f\"  → Ward: {row['Ward']}\")\n",
    "    print(f\"  → District: {row['District']}\")\n",
    "    print(f\"  → City: {row['City']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b03d26",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 3. FEATURE ENGINEERING\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "edde9841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3] FEATURE ENGINEERING\n",
      "================================================================================\n",
      "✓ Created: Price_per_m2\n",
      "✓ Created: Total_rooms\n",
      "✓ Created: Bedroom_Bathroom_ratio\n",
      "✓ Created: Area_per_floor\n",
      "✓ Created: Area_category\n",
      "✓ Created: Price_category\n",
      "✓ Created: 4 Has_* boolean flags\n",
      "\n",
      "✓ Total new features: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n[3] FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 3.1. Price per m²\n",
    "if 'Price' in df_processed.columns and 'Area' in df_processed.columns:\n",
    "    df_processed['Price_per_m2'] = df_processed['Price'] / df_processed['Area']\n",
    "    df_processed['Price_per_m2'] = df_processed['Price_per_m2'].replace([np.inf, -np.inf], np.nan)\n",
    "    print(\"✓ Created: Price_per_m2\")\n",
    "\n",
    "# 3.2. Total rooms\n",
    "if 'Bedrooms' in df_processed.columns and 'Bathrooms' in df_processed.columns:\n",
    "    df_processed['Total_rooms'] = df_processed['Bedrooms'].fillna(0) + df_processed['Bathrooms'].fillna(0)\n",
    "    print(\"✓ Created: Total_rooms\")\n",
    "\n",
    "\n",
    "# 3.3. Bedroom/Bathroom ratio\n",
    "if 'Bedrooms' in df_processed.columns and 'Bathrooms' in df_processed.columns:\n",
    "    df_processed['Bedroom_Bathroom_ratio'] = df_processed['Bedrooms'] / df_processed['Bathrooms'].replace(0, np.nan)\n",
    "    df_processed['Bedroom_Bathroom_ratio'] = df_processed['Bedroom_Bathroom_ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "    print(\"✓ Created: Bedroom_Bathroom_ratio\")\n",
    "\n",
    "# 3.4. Area per floor\n",
    "if 'Area' in df_processed.columns and 'Floors' in df_processed.columns:\n",
    "    df_processed['Area_per_floor'] = df_processed['Area'] / df_processed['Floors'].replace(0, np.nan)\n",
    "    df_processed['Area_per_floor'] = df_processed['Area_per_floor'].replace([np.inf, -np.inf], np.nan)\n",
    "    print(\"✓ Created: Area_per_floor\")\n",
    "\n",
    "# 3.5. Area category\n",
    "if 'Area' in df_processed.columns:\n",
    "    df_processed['Area_category'] = pd.cut(\n",
    "        df_processed['Area'],\n",
    "        bins=[0, 50, 80, 120, 200, np.inf],\n",
    "        labels=['Very Small', 'Small', 'Medium', 'Large', 'Very Large']\n",
    "    )\n",
    "    print(\"✓ Created: Area_category\")\n",
    "\n",
    "# 3.6. Price category\n",
    "if 'Price' in df_processed.columns:\n",
    "    df_processed['Price_category'] = pd.cut(\n",
    "        df_processed['Price'],\n",
    "        bins=[0, 5, 10, 20, 50, np.inf],\n",
    "        labels=['Very Cheap', 'Cheap', 'Medium', 'Expensive', 'Very Expensive']\n",
    "    )\n",
    "    print(\"✓ Created: Price_category\")\n",
    "\n",
    "# 3.7. Boolean flags\n",
    "df_processed['Has_Frontage'] = df_processed['Frontage'].notna().astype(int)\n",
    "df_processed['Has_Access_Road'] = df_processed['Access Road'].notna().astype(int)\n",
    "df_processed['Has_House_Direction'] = df_processed['House direction'].notna().astype(int)\n",
    "df_processed['Has_Balcony_Direction'] = df_processed['Balcony direction'].notna().astype(int)\n",
    "print(\"✓ Created: 4 Has_* boolean flags\")\n",
    "\n",
    "print(f\"\\n✓ Total new features: {len(df_processed.columns) - len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147766a",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 4. HANDLE MISSING VALUES\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cfe07bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4] HANDLING MISSING VALUES\n",
      "================================================================================\n",
      "\n",
      "Missing value categories:\n",
      "\n",
      "📊 Columns with >60% missing:\n",
      "           Column  Missing_Pct\n",
      "Balcony direction    87.354462\n",
      "  House direction    78.269274\n",
      "\n",
      "Dropping 2 columns:\n",
      "  ❌ Balcony direction\n",
      "  ❌ House direction\n",
      "\n",
      "📊 Columns with 20-60% missing:\n",
      "                Column  Missing_Pct\n",
      "           Access Road    47.804001\n",
      "       Furniture state    43.110811\n",
      "              Frontage    40.800180\n",
      "Bedroom_Bathroom_ratio    23.888514\n",
      "             Bathrooms    23.609800\n",
      "\n",
      "📊 Columns with <20% missing:\n",
      "        Column  Missing_Pct\n",
      "      Bedrooms    16.210384\n",
      "  Legal status    13.791863\n",
      "        Floors     9.408856\n",
      "Area_per_floor     9.408856\n",
      "\n",
      "--- Imputing numeric columns ---\n",
      "  ✓ Frontage: filled 9076 values with median 4.00\n",
      "  ✓ Access Road: filled 10634 values with median 5.00\n",
      "  ✓ Floors: filled 2093 values with median 4.00\n",
      "  ✓ Bedrooms: filled 3606 values with median 3.00\n",
      "  ✓ Bathrooms: filled 5252 values with median 3.00\n",
      "  ✓ Bedroom_Bathroom_ratio: filled 5314 values with median 1.00\n",
      "  ✓ Area_per_floor: filled 2093 values with median 13.00\n",
      "\n",
      "--- Imputing categorical columns ---\n",
      "  ✓ Legal status: filled 3068 values with mode 'Have certificate'\n",
      "  ✓ Furniture state: filled 9590 values with mode 'Full'\n",
      "\n",
      "✓ Remaining missing: 0\n",
      "\n",
      "✓ Saved: housing_data_processed.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n[4] HANDLING MISSING VALUES\")\n",
    "print(\"=\"*80)\n",
    "# Calculate missing percentages\n",
    "missing_pct = (df_processed.isnull().sum() / len(df_processed)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_pct.index,\n",
    "    'Missing_Pct': missing_pct.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Pct'] > 0].sort_values('Missing_Pct', ascending=False)\n",
    "\n",
    "print(\"\\nMissing value categories:\")\n",
    "\n",
    "# High missing (>60%)\n",
    "high_missing = missing_df[missing_df['Missing_Pct'] > 60]\n",
    "if not high_missing.empty:\n",
    "    print(f\"\\n📊 Columns with >60% missing:\")\n",
    "    print(high_missing.to_string(index=False))\n",
    "    \n",
    "    # Drop high missing columns (except Price)\n",
    "    cols_to_drop = [col for col in high_missing['Column'] if col != 'Price']\n",
    "    if cols_to_drop:\n",
    "        print(f\"\\nDropping {len(cols_to_drop)} columns:\")\n",
    "        for col in cols_to_drop:\n",
    "            print(f\"  ❌ {col}\")\n",
    "        df_processed = df_processed.drop(columns=cols_to_drop)\n",
    "# Medium missing (20-60%)\n",
    "medium_missing = missing_df[(missing_df['Missing_Pct'] > 20) & (missing_df['Missing_Pct'] <= 60)]\n",
    "if not medium_missing.empty:\n",
    "    print(f\"\\n📊 Columns with 20-60% missing:\")\n",
    "    print(medium_missing.to_string(index=False))\n",
    "\n",
    "# Low missing (<20%)\n",
    "low_missing = missing_df[missing_df['Missing_Pct'] <= 20]\n",
    "if not low_missing.empty:\n",
    "    print(f\"\\n📊 Columns with <20% missing:\")\n",
    "    print(low_missing.to_string(index=False))\n",
    "\n",
    "# Impute numeric columns\n",
    "print(\"\\n--- Imputing numeric columns ---\")\n",
    "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_missing = [col for col in numeric_cols if df_processed[col].isnull().sum() > 0]\n",
    "\n",
    "if numeric_missing:\n",
    "    for col in numeric_missing:\n",
    "        count = df_processed[col].isnull().sum()\n",
    "        median = df_processed[col].median()\n",
    "        df_processed[col].fillna(median, inplace=True)\n",
    "        print(f\"  ✓ {col}: filled {count} values with median {median:.2f}\")\n",
    "\n",
    "# Impute categorical columns\n",
    "print(\"\\n--- Imputing categorical columns ---\")\n",
    "categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "categorical_missing = [col for col in categorical_cols if df_processed[col].isnull().sum() > 0]\n",
    "\n",
    "if categorical_missing:\n",
    "    for col in categorical_missing:\n",
    "        count = df_processed[col].isnull().sum()\n",
    "        pct = (count / len(df_processed)) * 100\n",
    "        \n",
    "        if pct < 50:\n",
    "            mode = df_processed[col].mode()\n",
    "            if len(mode) > 0:\n",
    "                df_processed[col].fillna(mode[0], inplace=True)\n",
    "                print(f\"  ✓ {col}: filled {count} values with mode '{mode[0]}'\")\n",
    "        else:\n",
    "            df_processed[col].fillna('Unknown', inplace=True)\n",
    "            print(f\"  ✓ {col}: filled {count} values with 'Unknown'\")\n",
    "\n",
    "print(f\"\\n✓ Remaining missing: {df_processed.isnull().sum().sum()}\")\n",
    "\n",
    "# Save processed data\n",
    "df_processed.to_csv('../data/housing_data_processed.csv', index=False)\n",
    "print(\"\\n✓ Saved: housing_data_processed.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa17049",
   "metadata": {},
   "source": [
    "DEFINE PREPROCESSOR CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3c04bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RealEstatePreprocessor class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class RealEstatePreprocessor:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for real estate data.\n",
    "    Prevents data leakage by fitting only on training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_folds=5, random_state=42):\n",
    "        self.n_folds = n_folds\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Store encoders and scalers (FITTED ON TRAIN ONLY!)\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.target_encodings = {}\n",
    "        self.fill_values = {}\n",
    "        \n",
    "        # Features to drop (leakage)\n",
    "        self.leakage_cols = ['Price_per_m2', 'Price_category', 'Area_category', 'Ward']\n",
    "        \n",
    "        # Location scores\n",
    "        self.city_base_scores = {\n",
    "            'Hồ Chí Minh': 50,\n",
    "            'Hà Nội': 50,\n",
    "        }\n",
    "        \n",
    "        self.district_scores = {\n",
    "            'Hồ Chí Minh': {\n",
    "                'Quận 1': 10, 'Quận 3': 9, 'Bình Thạnh': 8, 'Phú Nhuận': 8,\n",
    "                'Quận 2': 7, 'Quận 7': 7, 'Quận 10': 7, 'Tân Bình': 7,\n",
    "                'Quận 5': 6, 'Quận 6': 6, 'Gò Vấp': 6, 'Quận 8': 5,\n",
    "                'Quận 9': 5, 'Thủ Đức': 5, 'Quận 12': 4, 'Tân Phú': 6,\n",
    "                'Bình Tân': 4, 'Bình Chánh': 4, 'Hóc Môn': 3,\n",
    "                'Củ Chi': 2, 'Nhà Bè': 3, 'Cần Giờ': 1\n",
    "            },\n",
    "            'Hà Nội': {\n",
    "                'Hoàn Kiếm': 10, 'Ba Đình': 9, 'Đống Đa': 8, 'Hai Bà Trưng': 8,\n",
    "                'Cầu Giấy': 7, 'Thanh Xuân': 7, 'Tây Hồ': 7,\n",
    "                'Long Biên': 6, 'Hoàng Mai': 6, 'Nam Từ Liêm': 6, 'Bắc Từ Liêm': 6,\n",
    "                'Hà Đông': 5, 'Đông Anh': 4, 'Gia Lâm': 4, 'Thanh Trì': 4,\n",
    "                'Sóc Sơn': 3, 'Ba Vì': 2, 'Mỹ Đức': 2, 'Chương Mỹ': 2,\n",
    "                'Thường Tín': 3, 'Mê Linh': 3, 'Hoài Đức': 3, 'Thạch Thất': 2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def clean_strings(self, df):\n",
    "        \"\"\"Step 1: Normalize string columns\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 1: STRING NORMALIZATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        string_cols = ['City', 'District']\n",
    "        for col in string_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].str.strip().str.title()\n",
    "                df[col] = df[col].str.replace(r'\\.$', '', regex=True)\n",
    "                print(f\"{col} unique values: {df[col].nunique()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_leakage(self, df):\n",
    "        \"\"\"Step 2: Remove leakage columns\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 2: REMOVE LEAKAGE FEATURES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        cols_to_drop = [col for col in self.leakage_cols if col in df.columns]\n",
    "        if cols_to_drop:\n",
    "            print(f\"Dropping: {cols_to_drop}\")\n",
    "            df = df.drop(columns=cols_to_drop)\n",
    "        return df\n",
    "    \n",
    "    def handle_missing(self, df, is_train=True):\n",
    "        \"\"\"Step 3: Handle missing values\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 3: HANDLE MISSING VALUES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Numeric: fill with median\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if df[col].isna().sum() > 0:\n",
    "                if is_train:\n",
    "                    self.fill_values[col] = df[col].median()\n",
    "                fill_val = self.fill_values.get(col, df[col].median())\n",
    "                df[col].fillna(fill_val, inplace=True)\n",
    "                if is_train:\n",
    "                    print(f\"✓ {col}: filled with TRAIN median {fill_val:.2f}\")\n",
    "                else:\n",
    "                    print(f\"✓ {col}: filled with STORED median {fill_val:.2f}\")\n",
    "        \n",
    "        # Categorical: fill with 'Unknown'\n",
    "        cat_cols = df.select_dtypes(include=['object']).columns\n",
    "        for col in cat_cols:\n",
    "            if df[col].isna().sum() > 0:\n",
    "                df[col].fillna('Unknown', inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_bad_records(self, df):\n",
    "        \"\"\"Step 4: Remove invalid records\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 4: REMOVE BAD RECORDS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        initial_shape = df.shape[0]\n",
    "        df = df[(df['Price'] > 0) & (df['Area'] > 0)]\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        print(f\"Removed {initial_shape - df.shape[0]} bad records\")\n",
    "        return df\n",
    "    \n",
    "    def handle_outliers(self, df, method='percentile'):\n",
    "        \"\"\"Step 5: Handle outliers\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 5: HANDLE OUTLIERS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        cols = ['Price', 'Area', 'Frontage', 'Access Road']\n",
    "        \n",
    "        for col in cols:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            lower = df[col].quantile(0.01)\n",
    "            upper = df[col].quantile(0.99)\n",
    "            df[col] = df[col].clip(lower, upper)\n",
    "            print(f\"{col}: capped to [{lower:.2f}, {upper:.2f}]\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"Step 6: Feature engineering\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 6: FEATURE ENGINEERING\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Basic features\n",
    "        if 'Total_rooms' not in df.columns:\n",
    "            df['Total_rooms'] = df['Bedrooms'] + df['Bathrooms']\n",
    "        \n",
    "        df['Bedroom_Bathroom_ratio'] = df['Bedrooms'] / df['Bathrooms'].replace(0, 1)\n",
    "        df['Area_per_floor'] = df['Area'] / df['Floors'].replace(0, 1)\n",
    "        df['Room_density'] = df['Total_rooms'] / df['Area']\n",
    "        \n",
    "        # Luxury score\n",
    "        binary_features = ['Has_Frontage', 'Has_Access_Road', \n",
    "                          'Has_House_Direction', 'Has_Balcony_Direction']\n",
    "        df['Luxury_score'] = sum(df[col] for col in binary_features if col in df.columns)\n",
    "        \n",
    "        # Location features\n",
    "        df['City_Base_Score'] = df['City'].map(self.city_base_scores).fillna(40)\n",
    "        df['District_Score'] = df.apply(\n",
    "            lambda row: self.district_scores.get(row['City'], {}).get(row['District'], 3),\n",
    "            axis=1\n",
    "        )\n",
    "        df['Location_Score'] = df['City_Base_Score'] + df['District_Score']\n",
    "        df['Location_Tier'] = pd.cut(\n",
    "            df['Location_Score'],\n",
    "            bins=[0, 50, 55, 60, 100],\n",
    "            labels=['Suburban', 'Urban', 'Premium', 'Elite']\n",
    "        )\n",
    "        \n",
    "        # Other features\n",
    "        df['Is_Apartment'] = df['Address'].str.contains('Dự án|Project', case=False, na=False).astype(int)\n",
    "        df['Full_Legal'] = (df['Legal status'] == 'Have Certificate').astype(int)\n",
    "        df['Full_Furniture'] = (df['Furniture state'] == 'Full').astype(int)\n",
    "        \n",
    "        print(f\"✓ Created 13 new features\")\n",
    "        return df\n",
    "    \n",
    "    def target_encode_kfold(self, df, col, target='Price', is_train=True):\n",
    "        \"\"\"Target encoding with K-Fold (prevents leakage)\"\"\"\n",
    "        if is_train:\n",
    "            # Initialize column\n",
    "            df[f'{col}_target_enc'] = 0.0\n",
    "            \n",
    "            kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "            \n",
    "            # Use iloc instead of loc to avoid index issues\n",
    "            for train_idx, val_idx in kf.split(df):\n",
    "                # Calculate mean target per category on train fold\n",
    "                train_data = df.iloc[train_idx]\n",
    "                target_means = train_data.groupby(col)[target].mean()\n",
    "                global_mean = train_data[target].mean()\n",
    "                \n",
    "                # Apply to validation fold using iloc\n",
    "                val_data = df.iloc[val_idx]\n",
    "                encoded_values = val_data[col].map(target_means).fillna(global_mean)\n",
    "                df.iloc[val_idx, df.columns.get_loc(f'{col}_target_enc')] = encoded_values.values\n",
    "            \n",
    "            # Store global encoding for test data\n",
    "            self.target_encodings[col] = df.groupby(col)[target].mean().to_dict()\n",
    "            self.target_encodings[f'{col}_global_mean'] = df[target].mean()\n",
    "            print(f\"  ✓ {col}: K-Fold encoded (train)\")\n",
    "        else:\n",
    "            global_mean = self.target_encodings.get(f'{col}_global_mean', 0)\n",
    "            df[f'{col}_target_enc'] = df[col].map(self.target_encodings[col]).fillna(global_mean)\n",
    "            print(f\"  ✓ {col}: Applied stored encoding (test)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def encode_categoricals(self, df, is_train=True):\n",
    "        \"\"\"Step 7: Encode categorical variables\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 7: ENCODE CATEGORICAL FEATURES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Target encoding\n",
    "        df = self.target_encode_kfold(df, 'District', is_train=is_train)\n",
    "        df = self.target_encode_kfold(df, 'City', is_train=is_train)\n",
    "        \n",
    "        # Label encoding\n",
    "        label_cols = ['Legal status', 'Furniture state', 'Location_Tier']\n",
    "        for col in label_cols:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            if is_train:\n",
    "                self.label_encoders[col] = LabelEncoder()\n",
    "                df[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df[col].astype(str))\n",
    "                print(f\"  ✓ {col}: Label encoded (train)\")\n",
    "            else:\n",
    "                df[f'{col}_encoded'] = df[col].astype(str).map(\n",
    "                    lambda x: self.label_encoders[col].transform([x])[0] \n",
    "                    if x in self.label_encoders[col].classes_ else -1\n",
    "                )\n",
    "                print(f\"  ✓ {col}: Applied stored encoding (test)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def scale_features(self, df, is_train=True):\n",
    "        \"\"\"Step 8: Scale numeric features\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 8: SCALE NUMERIC FEATURES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        exclude = ['Price', 'Address']\n",
    "        numeric_cols = [col for col in df.select_dtypes(include=[np.number]).columns \n",
    "                       if col not in exclude]\n",
    "        \n",
    "        if is_train:\n",
    "            df[numeric_cols] = self.scaler.fit_transform(df[numeric_cols])\n",
    "            print(f\"✓ Fitted scaler on {len(numeric_cols)} features (TRAIN)\")\n",
    "        else:\n",
    "            df[numeric_cols] = self.scaler.transform(df[numeric_cols])\n",
    "            print(f\"✓ Applied scaler to {len(numeric_cols)} features (TEST)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Complete preprocessing for TRAINING data\"\"\"\n",
    "        print(\"\\n\" + \"🚀 \" * 20)\n",
    "        print(\"PREPROCESSING PIPELINE - TRAINING DATA\")\n",
    "        print(\"🚀 \" * 20)\n",
    "        \n",
    "        # CRITICAL: Reset index to avoid KeyError in K-Fold\n",
    "        df = df.reset_index(drop=True)\n",
    "        print(\"✓ Index reset for safe processing\")\n",
    "        \n",
    "        df = self.clean_strings(df)\n",
    "        df = self.remove_leakage(df)\n",
    "        df = self.handle_missing(df, is_train=True)\n",
    "        df = self.remove_bad_records(df)\n",
    "        df = self.handle_outliers(df)\n",
    "        df = self.engineer_features(df)\n",
    "        df = self.encode_categoricals(df, is_train=True)\n",
    "        \n",
    "        # Separate target\n",
    "        y = df['Price'].copy()\n",
    "        X = df.drop(columns=['Price', 'Address', 'City', 'District', \n",
    "                             'Legal status', 'Furniture state', 'Location_Tier'], \n",
    "                    errors='ignore')\n",
    "        \n",
    "        X = self.scale_features(X, is_train=True)\n",
    "        \n",
    "        print(\"\\n\" + \"✅ \" * 20)\n",
    "        print(f\"FINAL SHAPE: X={X.shape}, y={y.shape}\")\n",
    "        print(\"✅ \" * 20)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"Apply preprocessing to TEST/NEW data\"\"\"\n",
    "        print(\"\\n\" + \"🔧 \" * 20)\n",
    "        print(\"PREPROCESSING PIPELINE - TEST DATA\")\n",
    "        print(\"🔧 \" * 20)\n",
    "        \n",
    "        # CRITICAL: Reset index to avoid KeyError\n",
    "        df = df.reset_index(drop=True)\n",
    "        print(\"✓ Index reset for safe processing\")\n",
    "        \n",
    "        df = self.clean_strings(df)\n",
    "        df = self.remove_leakage(df)\n",
    "        df = self.handle_missing(df, is_train=False)\n",
    "        df = self.engineer_features(df)\n",
    "        df = self.encode_categoricals(df, is_train=False)\n",
    "        \n",
    "        has_target = 'Price' in df.columns\n",
    "        if has_target:\n",
    "            y = df['Price'].copy()\n",
    "        \n",
    "        X = df.drop(columns=['Price', 'Address', 'City', 'District', \n",
    "                             'Legal status', 'Furniture state', 'Location_Tier'], \n",
    "                    errors='ignore')\n",
    "        X = self.scale_features(X, is_train=False)\n",
    "        \n",
    "        print(\"\\n\" + \"✅ \" * 20)\n",
    "        print(f\"FINAL SHAPE: X={X.shape}\")\n",
    "        print(\"✅ \" * 20)\n",
    "        \n",
    "        if has_target:\n",
    "            return X, y\n",
    "        return X\n",
    "\n",
    "print(\"✅ RealEstatePreprocessor class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e7a88",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e1afb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA LOADED\n",
      "================================================================================\n",
      "Total records: 22,245\n",
      "Total features: 23\n",
      "\n",
      "First few rows:\n",
      "                                             Address  Area  Frontage  \\\n",
      "0  Đường Nguyễn Văn Khối, Phường 11, Gò Vấp, Hồ C...  54.0       4.0   \n",
      "1   Đường Quang Trung, Phường 8, Gò Vấp, Hồ Chí Minh  92.0       4.0   \n",
      "2  Dự án Him Lam Thường Tín, Huyện Thường Tín, Hà...  74.0       5.0   \n",
      "\n",
      "   Access Road  Floors  Bedrooms  Bathrooms      Legal status Furniture state  \\\n",
      "0          3.5     2.0       2.0        3.0  Have certificate            Full   \n",
      "1          5.0     2.0       4.0        4.0  Have certificate            Full   \n",
      "2         18.0     5.0       4.0        5.0  Have certificate            Full   \n",
      "\n",
      "   Price  ... Price_per_m2 Total_rooms Bedroom_Bathroom_ratio  Area_per_floor  \\\n",
      "0   5.35  ...     0.099074         5.0               0.666667            27.0   \n",
      "1   6.90  ...     0.075000         8.0               1.000000            46.0   \n",
      "2   9.90  ...     0.133784         9.0               0.800000            14.8   \n",
      "\n",
      "   Area_category  Price_category  Has_Frontage Has_Access_Road  \\\n",
      "0          Small           Cheap             0               1   \n",
      "1         Medium           Cheap             0               0   \n",
      "2          Small           Cheap             1               1   \n",
      "\n",
      "  Has_House_Direction  Has_Balcony_Direction  \n",
      "0                   1                      1  \n",
      "1                   1                      1  \n",
      "2                   1                      1  \n",
      "\n",
      "[3 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/housing_data_processed.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total records: {df.shape[0]:,}\")\n",
    "print(f\"Total features: {df.shape[1]}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818de646",
   "metadata": {},
   "source": [
    "TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2d77823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAIN-TEST SPLIT\n",
      "================================================================================\n",
      "📊 Training set: 17,796 records (80.0%)\n",
      "📊 Test set:     4,449 records (20.0%)\n",
      "\n",
      "Price distribution:\n",
      "  Train - Mean: 6.18, Median: 6.20\n",
      "  Test  - Mean: 6.22, Median: 6.20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"📊 Training set: {train_df.shape[0]:,} records ({train_df.shape[0]/df.shape[0]*100:.1f}%)\")\n",
    "print(f\"📊 Test set:     {test_df.shape[0]:,} records ({test_df.shape[0]/df.shape[0]*100:.1f}%)\")\n",
    "print(f\"\\nPrice distribution:\")\n",
    "print(f\"  Train - Mean: {train_df['Price'].mean():.2f}, Median: {train_df['Price'].median():.2f}\")\n",
    "print(f\"  Test  - Mean: {test_df['Price'].mean():.2f}, Median: {test_df['Price'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1869b9e3",
   "metadata": {},
   "source": [
    "INITIALIZE PREPROCESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0af662a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPROCESSOR INITIALIZED\n",
      "================================================================================\n",
      "✓ K-Fold splits: 5\n",
      "✓ Random state: 42\n",
      "✓ Leakage columns to remove: ['Price_per_m2', 'Price_category', 'Area_category', 'Ward']\n",
      "✓ Cities covered: ['Hồ Chí Minh', 'Hà Nội']\n"
     ]
    }
   ],
   "source": [
    "preprocessor = RealEstatePreprocessor(\n",
    "    n_folds=5,        # Number of folds for target encoding\n",
    "    random_state=42   # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PREPROCESSOR INITIALIZED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ K-Fold splits: {preprocessor.n_folds}\")\n",
    "print(f\"✓ Random state: {preprocessor.random_state}\")\n",
    "print(f\"✓ Leakage columns to remove: {preprocessor.leakage_cols}\")\n",
    "print(f\"✓ Cities covered: {list(preprocessor.city_base_scores.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4768348b",
   "metadata": {},
   "source": [
    "FIT & TRANSFORM TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a8b58364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 \n",
      "PREPROCESSING PIPELINE - TRAINING DATA\n",
      "🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀 \n",
      "✓ Index reset for safe processing\n",
      "\n",
      "============================================================\n",
      "STEP 1: STRING NORMALIZATION\n",
      "============================================================\n",
      "City unique values: 4\n",
      "District unique values: 73\n",
      "\n",
      "============================================================\n",
      "STEP 2: REMOVE LEAKAGE FEATURES\n",
      "============================================================\n",
      "Dropping: ['Price_per_m2', 'Price_category', 'Area_category', 'Ward']\n",
      "\n",
      "============================================================\n",
      "STEP 3: HANDLE MISSING VALUES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STEP 4: REMOVE BAD RECORDS\n",
      "============================================================\n",
      "Removed 24 bad records\n",
      "\n",
      "============================================================\n",
      "STEP 5: HANDLE OUTLIERS\n",
      "============================================================\n",
      "Price: capped to [1.68, 10.00]\n",
      "Area: capped to [20.00, 160.00]\n",
      "Frontage: capped to [3.00, 12.00]\n",
      "Access Road: capped to [2.00, 21.00]\n",
      "\n",
      "============================================================\n",
      "STEP 6: FEATURE ENGINEERING\n",
      "============================================================\n",
      "✓ Created 13 new features\n",
      "\n",
      "============================================================\n",
      "STEP 7: ENCODE CATEGORICAL FEATURES\n",
      "============================================================\n",
      "  ✓ District: K-Fold encoded (train)\n",
      "  ✓ City: K-Fold encoded (train)\n",
      "  ✓ Legal status: Label encoded (train)\n",
      "  ✓ Furniture state: Label encoded (train)\n",
      "  ✓ Location_Tier: Label encoded (train)\n",
      "\n",
      "============================================================\n",
      "STEP 8: SCALE NUMERIC FEATURES\n",
      "============================================================\n",
      "✓ Fitted scaler on 26 features (TRAIN)\n",
      "\n",
      "✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ \n",
      "FINAL SHAPE: X=(17772, 26), y=(17772,)\n",
      "✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ \n",
      "\n",
      "================================================================================\n",
      "TRAINING DATA PREPROCESSING COMPLETE\n",
      "================================================================================\n",
      "✅ X_train shape: (17772, 26)\n",
      "✅ y_train shape: (17772,)\n",
      "\n",
      "Feature names (26):\n",
      "   1. Area\n",
      "   2. Frontage\n",
      "   3. Access Road\n",
      "   4. Floors\n",
      "   5. Bedrooms\n",
      "   6. Bathrooms\n",
      "   7. Total_rooms\n",
      "   8. Bedroom_Bathroom_ratio\n",
      "   9. Area_per_floor\n",
      "  10. Has_Frontage\n",
      "  11. Has_Access_Road\n",
      "  12. Has_House_Direction\n",
      "  13. Has_Balcony_Direction\n",
      "  14. Room_density\n",
      "  15. Luxury_score\n",
      "  16. City_Base_Score\n",
      "  17. District_Score\n",
      "  18. Location_Score\n",
      "  19. Is_Apartment\n",
      "  20. Full_Legal\n",
      "  21. Full_Furniture\n",
      "  22. District_target_enc\n",
      "  23. City_target_enc\n",
      "  24. Legal status_encoded\n",
      "  25. Furniture state_encoded\n",
      "  26. Location_Tier_encoded\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = preprocessor.fit_transform(train_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING DATA PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✅ X_train shape: {X_train.shape}\")\n",
    "print(f\"✅ y_train shape: {y_train.shape}\")\n",
    "print(f\"\\nFeature names ({len(X_train.columns)}):\")\n",
    "for i, col in enumerate(X_train.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e1bd5",
   "metadata": {},
   "source": [
    "TRANSFORM TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8c3e0c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 \n",
      "PREPROCESSING PIPELINE - TEST DATA\n",
      "🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 🔧 \n",
      "✓ Index reset for safe processing\n",
      "\n",
      "============================================================\n",
      "STEP 1: STRING NORMALIZATION\n",
      "============================================================\n",
      "City unique values: 5\n",
      "District unique values: 63\n",
      "\n",
      "============================================================\n",
      "STEP 2: REMOVE LEAKAGE FEATURES\n",
      "============================================================\n",
      "Dropping: ['Price_per_m2', 'Price_category', 'Area_category', 'Ward']\n",
      "\n",
      "============================================================\n",
      "STEP 3: HANDLE MISSING VALUES\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STEP 6: FEATURE ENGINEERING\n",
      "============================================================\n",
      "✓ Created 13 new features\n",
      "\n",
      "============================================================\n",
      "STEP 7: ENCODE CATEGORICAL FEATURES\n",
      "============================================================\n",
      "  ✓ District: Applied stored encoding (test)\n",
      "  ✓ City: Applied stored encoding (test)\n",
      "  ✓ Legal status: Applied stored encoding (test)\n",
      "  ✓ Furniture state: Applied stored encoding (test)\n",
      "  ✓ Location_Tier: Applied stored encoding (test)\n",
      "\n",
      "============================================================\n",
      "STEP 8: SCALE NUMERIC FEATURES\n",
      "============================================================\n",
      "✓ Applied scaler to 26 features (TEST)\n",
      "\n",
      "✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ \n",
      "FINAL SHAPE: X=(4449, 26)\n",
      "✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ \n",
      "\n",
      "================================================================================\n",
      "TEST DATA PREPROCESSING COMPLETE\n",
      "================================================================================\n",
      "✅ X_test shape: (4449, 26)\n",
      "✅ y_test shape: (4449,)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = preprocessor.transform(test_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST DATA PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✅ X_test shape: {X_test.shape}\")\n",
    "print(f\"✅ y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f81ca",
   "metadata": {},
   "source": [
    "VERIFY NO DATA LEAKAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b0ccaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 \n",
      "DATA LEAKAGE VERIFICATION (CORRECTED)\n",
      "🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 🔍 \n",
      "\n",
      "1. Feature consistency:\n",
      "   ✅ Train and test have SAME features\n",
      "   Total features: 26\n",
      "\n",
      "2. Target variable:\n",
      "   ✅ Price NOT in features (correct!)\n",
      "\n",
      "3. Stored preprocessing values:\n",
      "   ✅ Fill values stored: 0 numeric columns\n",
      "   ✅ Label encoders stored: 3 categorical columns\n",
      "      Columns: ['Legal status', 'Furniture state', 'Location_Tier']\n",
      "   ✅ Target encodings stored: 2 columns\n",
      "      Columns: ['District', 'City']\n",
      "   ✅ Scaler fitted: 26 features\n",
      "\n",
      "4. Target encoding verification:\n",
      "   Checking that test uses stored train encodings...\n",
      "\n",
      "   Stored District encodings (from train):\n",
      "      Ba Vì: 8.43\n",
      "      Ba Đình: 7.29\n",
      "      Bình Chánh: 5.28\n",
      "\n",
      "   Verification for District='Huyện Quốc Oai':\n",
      "      Stored (from train): 3.060000\n",
      "      Applied (in test):   -3.909154\n",
      "      ❌ MISMATCH! Possible leakage!\n",
      "\n",
      "5. Scaler verification:\n",
      "   Train data range (sample feature):\n",
      "      Area: [-1.3661, 4.0915]\n",
      "   Test data range (same feature):\n",
      "      Area: [-2.0248, 17.3454]\n",
      "   ℹ️  Test range can exceed train range (this is normal)\n",
      "\n",
      "6. Categorical columns removed:\n",
      "   ✅ Original categorical columns removed\n",
      "      Removed: ['City', 'District', 'Legal status', 'Furniture state', 'Location_Tier']\n",
      "\n",
      "7. Leakage columns removed:\n",
      "   ✅ All leakage columns removed\n",
      "      Target leakage columns: ['Price_per_m2', 'Price_category', 'Area_category', 'Ward']\n",
      "\n",
      "============================================================\n",
      "VERIFICATION SUMMARY\n",
      "============================================================\n",
      "⚠️  Some checks failed - review warnings above\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"🔍 \" * 20)\n",
    "print(\"DATA LEAKAGE VERIFICATION (CORRECTED)\")\n",
    "print(\"🔍 \" * 20)\n",
    "\n",
    "# Check 1: Feature names match\n",
    "print(\"\\n1. Feature consistency:\")\n",
    "if list(X_train.columns) == list(X_test.columns):\n",
    "    print(\"   ✅ Train and test have SAME features\")\n",
    "    print(f\"   Total features: {len(X_train.columns)}\")\n",
    "else:\n",
    "    print(\"   ❌ WARNING: Feature mismatch!\")\n",
    "    missing_in_test = set(X_train.columns) - set(X_test.columns)\n",
    "    missing_in_train = set(X_test.columns) - set(X_train.columns)\n",
    "    if missing_in_test:\n",
    "        print(f\"   Missing in test: {missing_in_test}\")\n",
    "    if missing_in_train:\n",
    "        print(f\"   Missing in train: {missing_in_train}\")\n",
    "\n",
    "# Check 2: No Price in features\n",
    "print(\"\\n2. Target variable:\")\n",
    "if 'Price' not in X_train.columns and 'Price' not in X_test.columns:\n",
    "    print(\"   ✅ Price NOT in features (correct!)\")\n",
    "else:\n",
    "    print(\"   ❌ WARNING: Price found in features!\")\n",
    "\n",
    "# Check 3: Verify stored values\n",
    "print(\"\\n3. Stored preprocessing values:\")\n",
    "print(f\"   ✅ Fill values stored: {len(preprocessor.fill_values)} numeric columns\")\n",
    "if len(preprocessor.fill_values) > 0:\n",
    "    print(f\"      Example: {list(preprocessor.fill_values.items())[:2]}\")\n",
    "\n",
    "print(f\"   ✅ Label encoders stored: {len(preprocessor.label_encoders)} categorical columns\")\n",
    "if len(preprocessor.label_encoders) > 0:\n",
    "    print(f\"      Columns: {list(preprocessor.label_encoders.keys())}\")\n",
    "\n",
    "target_enc_cols = [k for k in preprocessor.target_encodings.keys() if not k.endswith('_global_mean')]\n",
    "print(f\"   ✅ Target encodings stored: {len(target_enc_cols)} columns\")\n",
    "if len(target_enc_cols) > 0:\n",
    "    print(f\"      Columns: {target_enc_cols}\")\n",
    "\n",
    "print(f\"   ✅ Scaler fitted: {preprocessor.scaler.n_features_in_} features\")\n",
    "\n",
    "# Check 4: CORRECTED - Compare stored encoding values directly\n",
    "print(\"\\n4. Target encoding verification:\")\n",
    "print(\"   Checking that test uses stored train encodings...\")\n",
    "\n",
    "if 'District' in preprocessor.target_encodings:\n",
    "    district_encodings = preprocessor.target_encodings['District']\n",
    "    \n",
    "    # Show sample of stored encodings\n",
    "    sample_districts = list(district_encodings.items())[:3]\n",
    "    print(f\"\\n   Stored District encodings (from train):\")\n",
    "    for district, encoding in sample_districts:\n",
    "        print(f\"      {district}: {encoding:.2f}\")\n",
    "    \n",
    "    # Verify test data uses these exact values\n",
    "    if 'District_target_enc' in X_test.columns:\n",
    "        # Get a sample district from test_df that exists in training\n",
    "        common_districts = set(test_df['District']) & set(district_encodings.keys())\n",
    "        if common_districts:\n",
    "            sample_district = list(common_districts)[0]\n",
    "            stored_encoding = district_encodings[sample_district]\n",
    "            \n",
    "            # Find this encoding in X_test\n",
    "            test_indices = test_df.index[test_df['District'] == sample_district].tolist()\n",
    "            if test_indices:\n",
    "                # Get the first matching index in test_df\n",
    "                original_idx = test_indices[0]\n",
    "                # Find corresponding position in X_test (which was reset to 0-based index)\n",
    "                position_in_test = test_df.index.get_loc(original_idx)\n",
    "                actual_test_encoding = X_test.iloc[position_in_test]['District_target_enc']\n",
    "                \n",
    "                print(f\"\\n   Verification for District='{sample_district}':\")\n",
    "                print(f\"      Stored (from train): {stored_encoding:.6f}\")\n",
    "                print(f\"      Applied (in test):   {actual_test_encoding:.6f}\")\n",
    "                \n",
    "                if abs(stored_encoding - actual_test_encoding) < 1e-6:\n",
    "                    print(f\"      ✅ MATCH! Test uses train encoding\")\n",
    "                else:\n",
    "                    print(f\"      ❌ MISMATCH! Possible leakage!\")\n",
    "\n",
    "# Check 5: Scaler verification\n",
    "print(\"\\n5. Scaler verification:\")\n",
    "print(f\"   Train data range (sample feature):\")\n",
    "sample_feature = X_train.columns[0]\n",
    "print(f\"      {sample_feature}: [{X_train[sample_feature].min():.4f}, {X_train[sample_feature].max():.4f}]\")\n",
    "print(f\"   Test data range (same feature):\")\n",
    "print(f\"      {sample_feature}: [{X_test[sample_feature].min():.4f}, {X_test[sample_feature].max():.4f}]\")\n",
    "print(f\"   ℹ️  Test range can exceed train range (this is normal)\")\n",
    "\n",
    "# Check 6: No raw categorical columns in features\n",
    "print(\"\\n6. Categorical columns removed:\")\n",
    "categorical_originals = ['City', 'District', 'Legal status', 'Furniture state', 'Location_Tier']\n",
    "found_in_train = [col for col in categorical_originals if col in X_train.columns]\n",
    "found_in_test = [col for col in categorical_originals if col in X_test.columns]\n",
    "\n",
    "if not found_in_train and not found_in_test:\n",
    "    print(f\"   ✅ Original categorical columns removed\")\n",
    "    print(f\"      Removed: {categorical_originals}\")\n",
    "else:\n",
    "    print(f\"   ❌ WARNING: Original categorical columns still present!\")\n",
    "    if found_in_train:\n",
    "        print(f\"      In train: {found_in_train}\")\n",
    "    if found_in_test:\n",
    "        print(f\"      In test: {found_in_test}\")\n",
    "\n",
    "# Check 7: Leakage columns removed\n",
    "print(\"\\n7. Leakage columns removed:\")\n",
    "leakage_in_train = [col for col in preprocessor.leakage_cols if col in X_train.columns]\n",
    "leakage_in_test = [col for col in preprocessor.leakage_cols if col in X_test.columns]\n",
    "\n",
    "if not leakage_in_train and not leakage_in_test:\n",
    "    print(f\"   ✅ All leakage columns removed\")\n",
    "    print(f\"      Target leakage columns: {preprocessor.leakage_cols}\")\n",
    "else:\n",
    "    print(f\"   ❌ WARNING: Leakage columns still present!\")\n",
    "    if leakage_in_train:\n",
    "        print(f\"      In train: {leakage_in_train}\")\n",
    "    if leakage_in_test:\n",
    "        print(f\"      In test: {leakage_in_test}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_checks_passed = (\n",
    "    list(X_train.columns) == list(X_test.columns) and\n",
    "    'Price' not in X_train.columns and\n",
    "    'Price' not in X_test.columns and\n",
    "    len(preprocessor.fill_values) > 0 and\n",
    "    len(preprocessor.label_encoders) > 0 and\n",
    "    not found_in_train and\n",
    "    not found_in_test and\n",
    "    not leakage_in_train and\n",
    "    not leakage_in_test\n",
    ")\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(\"✅ ALL CHECKS PASSED - No data leakage detected!\")\n",
    "    print(\"✅ Ready for model training\")\n",
    "else:\n",
    "    print(\"⚠️  Some checks failed - review warnings above\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6ec0f",
   "metadata": {},
   "source": [
    "FINAL SUMMARY & NEXT STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "639b09c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 \n",
      "PREPROCESSING SUMMARY\n",
      "🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 🎯 \n",
      "\n",
      "📊 Dataset sizes:\n",
      "   Train: 17,772 samples × 26 features\n",
      "   Test:  4,449 samples × 26 features\n",
      "\n",
      "🔒 Data leakage prevention:\n",
      "   ✅ Scaler fitted ONLY on train, applied to test\n",
      "   ✅ Encoders fitted ONLY on train, applied to test\n",
      "   ✅ Missing value medians from train ONLY\n",
      "   ✅ Target encoding with K-Fold (train) → stored (test)\n",
      "\n",
      "📈 Ready for modeling:\n",
      "   ✅ All features scaled to same range\n",
      "   ✅ All categorical variables encoded\n",
      "   ✅ No missing values\n",
      "   ✅ No data leakage\n",
      "\n",
      "🚀 Next steps:\n",
      "   1. Train your model on X_train, y_train\n",
      "   2. Evaluate on X_test, y_test\n",
      "   3. For new data: X_new = preprocessor.transform(new_df)\n",
      "\n",
      "================================================================================\n",
      "ALL PREPROCESSING COMPLETE! 🎉\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"🎯 \" * 20)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"🎯 \" * 20)\n",
    "\n",
    "print(\"\\n📊 Dataset sizes:\")\n",
    "print(f\"   Train: {X_train.shape[0]:,} samples × {X_train.shape[1]} features\")\n",
    "print(f\"   Test:  {X_test.shape[0]:,} samples × {X_test.shape[1]} features\")\n",
    "\n",
    "print(\"\\n🔒 Data leakage prevention:\")\n",
    "print(\"   ✅ Scaler fitted ONLY on train, applied to test\")\n",
    "print(\"   ✅ Encoders fitted ONLY on train, applied to test\")\n",
    "print(\"   ✅ Missing value medians from train ONLY\")\n",
    "print(\"   ✅ Target encoding with K-Fold (train) → stored (test)\")\n",
    "\n",
    "print(\"\\n📈 Ready for modeling:\")\n",
    "print(\"   ✅ All features scaled to same range\")\n",
    "print(\"   ✅ All categorical variables encoded\")\n",
    "print(\"   ✅ No missing values\")\n",
    "print(\"   ✅ No data leakage\")\n",
    "\n",
    "print(\"\\n🚀 Next steps:\")\n",
    "print(\"   1. Train your model on X_train, y_train\")\n",
    "print(\"   2. Evaluate on X_test, y_test\")\n",
    "print(\"   3. For new data: X_new = preprocessor.transform(new_df)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL PREPROCESSING COMPLETE! 🎉\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
